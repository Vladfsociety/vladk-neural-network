[tool.poetry]
name = "vladk-neural-network"
version = "0.1.0"
description = "A description of your project"
authors = ["Your Name <you@example.com>"]

[tool.poetry.dependencies]
python = "^3.8"
# Add other dependencies here

[tool.poetry.packages]
include = ["vladk_neural_network"]


test_predictions
[[tensor([0.0133])], [tensor([3.4210])], [tensor([1.7721])]]

test_predictions
[[tensor([-0.0902])], [tensor([2.8185])], [tensor([1.4110])]]


[[tensor([-0.2594])], [tensor([1.7169])], [tensor([0.7606])]]


[[tensor([-0.2912])], [tensor([1.2591])], [tensor([0.5090])]]

[[tensor([-0.1964])],
 [tensor([1.0806])],
 [tensor([0.4627])],
 [tensor([0.6686])]]

 [[tensor([-0.0863])],
 [tensor([1.0168])],
 [tensor([0.4830])],
 [tensor([0.6609])]]

 [[tensor([-0.0070])],
 [tensor([0.9716])],
 [tensor([0.4981])],
 [tensor([0.6560])]]


  /home/vlad/Documents/Projects/vladk-neural-network/src/main.py:45: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
/home/vlad/Documents/Projects/vladk-neural-network/src/model/base.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.loss.backward(self.optimizer, self._layers, torch.tensor(predict), torch.tensor(train_sample['output']))
/home/vlad/Documents/Projects/vladk-neural-network/src/main.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(x)
/home/vlad/Documents/Projects/vladk-neural-network/src/main.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(y)
/home/vlad/Documents/Projects/vladk-neural-network/src/main.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  z = torch.tensor(z)
/home/vlad/Documents/Projects/vladk-neural-network/src/main.py:72: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
/home/vlad/Documents/Projects/vladk-neural-network/src/main.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(x)
/home/vlad/Documents/Projects/vladk-neural-network/src/main.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(y)
/home/vlad/Documents/Projects/vladk-neural-network/src/main.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  z = torch.tensor(z)
/home/vlad/Documents/Projects/vladk-neural-network/src/main.py:72: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()




  if no use output layer
  poetry run python src/main.py > output.txt
/home/vlad/Documents/Projects/vladk-neural-network/src/model/base.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.loss.backward(self.optimizer, self._layers, torch.tensor(predict), torch.tensor(train_sample['output']))
Traceback (most recent call last):
  File "/home/vlad/Documents/Projects/vladk-neural-network/src/main.py", line 188, in <module>
    nn.fit(train_dataset, test_dataset, epochs=epochs)
  File "/home/vlad/Documents/Projects/vladk-neural-network/src/model/base.py", line 95, in fit
    loss = self.loss.calculate(torch.tensor(self._prediction), torch.tensor(self._actual))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars


Первым делом добавить тесты





batch 1 
Epoch: 49/50, Loss: 0.0113, r2 score: 0.9987
Test dataset validation. Loss: 0.0139, r2 score: 0.9984
--- 16.43903636932373 seconds ---


batch 2
Epoch: 49/50, Loss: 0.1676, r2 score: 0.9803
Test dataset validation. Loss: 0.1775, r2 score: 0.9801
--- 16.935725212097168 seconds ---

batch 1
Epoch: 49/50, Loss: 0.0095, r2 score: 0.9989
Test dataset validation. Loss: 0.0114, r2 score: 0.9987
--- 16.717719554901123 seconds ---

batch 4
Epoch: 49/50, Loss: 5.8183, r2 score: 0.3156
Test dataset validation. Loss: 5.9799, r2 score: 0.3287
--- 18.0739266872406 seconds ---

batch 4 lr 0.01
Epoch: 49/50, Loss: 0.0004, r2 score: 1.0
Test dataset validation. Loss: 0.0005, r2 score: 0.9999
--- 17.332524061203003 seconds ---

batch 1 lr 0.01
Epoch: 49/50, Loss: 0.001, r2 score: 0.9999
Test dataset validation. Loss: 0.0062, r2 score: 0.9993
--- 15.673651456832886 seconds ---


more complic model
batch 1
Epoch: 99/100, Loss: 0.4454, r2 score: 0.9908
Test dataset validation. Loss: 0.7514, r2 score: 0.9854
--- 3.9565186500549316 seconds ---


batch 4
Epoch: 99/100, Loss: 1.1448, r2 score: 0.9764
Test dataset validation. Loss: 1.313, r2 score: 0.9745
--- 3.285139322280884 seconds ---

batch 8
Epoch: 99/100, Loss: 1.3276, r2 score: 0.9726
Test dataset validation. Loss: 1.424, r2 score: 0.9723
--- 3.2789177894592285 seconds ---

batch 16
Epoch: 99/100, Loss: 2.0787, r2 score: 0.9571
Test dataset validation. Loss: 2.0876, r2 score: 0.9594
--- 3.0714268684387207 seconds ---


3d quad
batch 1
Epoch: 19/20, Loss: 0.0006, r2 score: 0.9999
Test dataset validation. Loss: 0.1335, r2 score: 0.9736
--- 423.70688581466675 seconds ---

batch 32
Epoch: 20/20, Loss: 0.0146, r2 score: 0.9969
Test dataset validation. Loss: 0.0601, r2 score: 0.9881
--- 156.04590106010437 seconds ---

batch 64
Epoch: 20/20, Loss: 0.0275, r2 score: 0.9941
Test dataset validation. Loss: 0.0635, r2 score: 0.9875
--- 138.64127945899963 seconds ---

batch 128
Epoch: 20/20, Loss: 0.0824, r2 score: 0.9822
Test dataset validation. Loss: 0.1834, r2 score: 0.9638
--- 153.09762501716614 seconds ---

batch 256
Epoch: 20/20, Loss: 1.0324, r2 score: 0.7767
Test dataset validation. Loss: 1.2249, r2 score: 0.758
--- 144.80464816093445 seconds ---

batch 64
Epoch: 20/20, Loss: 0.032, r2 score: 0.9931
Test dataset validation. Loss: 0.0784, r2 score: 0.9845
--- 148.19429850578308 seconds ---

batch 64 shuffle
Epoch: 20/20, Loss: 0.0283, r2 score: 0.9939
Test dataset validation. Loss: 0.0329, r2 score: 0.9935
--- 156.4179859161377 seconds --







Epoch: 40/40, Loss: 0.5841, R2 score: 0.9759
Test dataset validation. Loss: 0.6434, R2 score: 0.975
--- 16.375547409057617 seconds ---


Epoch: 40/40, Loss: 0.5669, R2 score: 0.9766
Test dataset validation. Loss: 0.6277, R2 score: 0.9756
--- 14.545613050460815 seconds ---


'b': tensor([[ 0.3932],
        [ 0.2622],
        [-0.1065],
        [ 0.1861],
        [ 0.4144],
        [ 0.2395],
        [-0.5613],
        [ 0.1152],
        [-0.1406],
        [ 0.4850],
        [-0.5589],
        [ 0.5319],
        [ 0.3562],
        [-0.1339],
        [ 0.2887],
        [-0.3296]]),

'w': tensor([[-0.1276, -0.2779,  0.4979,  0.5601,  0.2960, -0.2253,  0.2908,  0.1090,
          0.4300, -0.0269,  0.3568, -0.3834, -0.0432,  0.3246, -0.4088,  0.5653]]),



'b': tensor([[ 0.4043],
        [ 0.3085],
        [-0.1065],
        [ 0.1836],
        [ 0.3595],
        [ 0.1364],
        [-0.5613],
        [ 0.1327],
        [-0.1406],
        [ 0.4958],
        [-0.5589],
        [ 0.4930],
        [ 0.3183],
        [-0.1339],
        [ 0.2830],
        [-0.3296]])

'w': tensor([[-0.1167, -0.2779,  0.4979,  0.6723,  0.2960, -0.2881,  0.2908,  0.1792,
          0.4300, -0.0454,  0.3364, -0.3834, -0.0432,  0.3246, -0.4088,  0.6313]]),


Похоже что стоит попробовать с leaky relu







Так на ирисе воспроизводится хороший но не идеальній результат
layers = [
    FullyConnected(4, Relu()),
    FullyConnected(1, Sigmoid())
]
nn = NeuralNetwork(
    Input(4),
    layers,
    optimizer=SGD(learning_rate=0.001),
    loss=BinaryCrossEntropy(),
    metric=Accuracy(),
    convert_prediction='binary'
)

epochs = 30
nn.fit(train_dataset, test_dataset, epochs=epochs, batch_size=1, verbose=True)



















============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.3.2, pluggy-1.5.0
rootdir: /home/vlad/Documents/Projects/vladk-neural-network
configfile: pyproject.toml
testpaths: tests
collected 9 items

tests/test_binary_classification.py 
Binary classification. Testing Iris-setosa - Iris-versicolor combination
Epoch: 1/50, Loss: 0.66, Accuracy: 0.5857
Epoch: 2/50, Loss: 0.6436, Accuracy: 0.5857
Epoch: 3/50, Loss: 0.63, Accuracy: 0.5857
Epoch: 4/50, Loss: 0.6182, Accuracy: 0.5857
Epoch: 5/50, Loss: 0.6062, Accuracy: 0.5857
Epoch: 6/50, Loss: 0.5933, Accuracy: 0.5857
Epoch: 7/50, Loss: 0.5797, Accuracy: 0.5857
Epoch: 8/50, Loss: 0.5651, Accuracy: 0.5857
Epoch: 9/50, Loss: 0.5498, Accuracy: 0.5857
Epoch: 10/50, Loss: 0.5336, Accuracy: 0.5857
Epoch: 11/50, Loss: 0.5162, Accuracy: 0.7714
Epoch: 12/50, Loss: 0.4976, Accuracy: 0.7286
Epoch: 13/50, Loss: 0.478, Accuracy: 0.9571
Epoch: 14/50, Loss: 0.4571, Accuracy: 0.9714
Epoch: 15/50, Loss: 0.4345, Accuracy: 1.0
Epoch: 16/50, Loss: 0.4117, Accuracy: 1.0
Epoch: 17/50, Loss: 0.3879, Accuracy: 1.0
Epoch: 18/50, Loss: 0.3641, Accuracy: 1.0
Epoch: 19/50, Loss: 0.34, Accuracy: 1.0
Epoch: 20/50, Loss: 0.3161, Accuracy: 1.0
Epoch: 21/50, Loss: 0.2922, Accuracy: 1.0
Epoch: 22/50, Loss: 0.2698, Accuracy: 1.0
Epoch: 23/50, Loss: 0.2479, Accuracy: 1.0
Epoch: 24/50, Loss: 0.2274, Accuracy: 1.0
Epoch: 25/50, Loss: 0.208, Accuracy: 1.0
Epoch: 26/50, Loss: 0.1905, Accuracy: 1.0
Epoch: 27/50, Loss: 0.1742, Accuracy: 1.0
Epoch: 28/50, Loss: 0.1592, Accuracy: 1.0
Epoch: 29/50, Loss: 0.1458, Accuracy: 1.0
Epoch: 30/50, Loss: 0.1335, Accuracy: 1.0
Epoch: 31/50, Loss: 0.1225, Accuracy: 1.0
Epoch: 32/50, Loss: 0.1125, Accuracy: 1.0
Epoch: 33/50, Loss: 0.1035, Accuracy: 1.0
Epoch: 34/50, Loss: 0.0954, Accuracy: 1.0
Epoch: 35/50, Loss: 0.0881, Accuracy: 1.0
Epoch: 36/50, Loss: 0.0815, Accuracy: 1.0
Epoch: 37/50, Loss: 0.0756, Accuracy: 1.0
Epoch: 38/50, Loss: 0.0703, Accuracy: 1.0
Epoch: 39/50, Loss: 0.0654, Accuracy: 1.0
Epoch: 40/50, Loss: 0.0611, Accuracy: 1.0
Epoch: 41/50, Loss: 0.0572, Accuracy: 1.0
Epoch: 42/50, Loss: 0.0536, Accuracy: 1.0
Epoch: 43/50, Loss: 0.0504, Accuracy: 1.0
Epoch: 44/50, Loss: 0.0474, Accuracy: 1.0
Epoch: 45/50, Loss: 0.0447, Accuracy: 1.0
Epoch: 46/50, Loss: 0.0422, Accuracy: 1.0
Epoch: 47/50, Loss: 0.0399, Accuracy: 1.0
Epoch: 48/50, Loss: 0.0378, Accuracy: 1.0
Epoch: 49/50, Loss: 0.0359, Accuracy: 1.0
Epoch: 50/50, Loss: 0.0341, Accuracy: 1.0
BCE: 0.04, Accuracy: 1.0, Fit time: 1.229 seconds
.
Binary classification. Testing Iris-versicolor - Iris-virginica combination
Epoch: 1/50, Loss: 0.6893, Accuracy: 0.4714
Epoch: 2/50, Loss: 0.6693, Accuracy: 0.7857
Epoch: 3/50, Loss: 0.6553, Accuracy: 0.7
Epoch: 4/50, Loss: 0.6437, Accuracy: 0.7143
Epoch: 5/50, Loss: 0.6349, Accuracy: 0.5857
Epoch: 6/50, Loss: 0.6271, Accuracy: 0.6
Epoch: 7/50, Loss: 0.6199, Accuracy: 0.5857
Epoch: 8/50, Loss: 0.6129, Accuracy: 0.6
Epoch: 9/50, Loss: 0.6062, Accuracy: 0.6286
Epoch: 10/50, Loss: 0.5992, Accuracy: 0.6857
Epoch: 11/50, Loss: 0.592, Accuracy: 0.6857
Epoch: 12/50, Loss: 0.5844, Accuracy: 0.6429
Epoch: 13/50, Loss: 0.5773, Accuracy: 0.7714
Epoch: 14/50, Loss: 0.5697, Accuracy: 0.7429
Epoch: 15/50, Loss: 0.5618, Accuracy: 0.7571
Epoch: 16/50, Loss: 0.5537, Accuracy: 0.7571
Epoch: 17/50, Loss: 0.5454, Accuracy: 0.8
Epoch: 18/50, Loss: 0.5367, Accuracy: 0.8143
Epoch: 19/50, Loss: 0.5278, Accuracy: 0.8143
Epoch: 20/50, Loss: 0.5189, Accuracy: 0.8143
Epoch: 21/50, Loss: 0.5095, Accuracy: 0.8286
Epoch: 22/50, Loss: 0.4997, Accuracy: 0.8857
Epoch: 23/50, Loss: 0.4898, Accuracy: 0.8571
Epoch: 24/50, Loss: 0.4802, Accuracy: 0.8714
Epoch: 25/50, Loss: 0.4697, Accuracy: 0.8857
Epoch: 26/50, Loss: 0.4595, Accuracy: 0.8857
Epoch: 27/50, Loss: 0.4497, Accuracy: 0.8714
Epoch: 28/50, Loss: 0.4392, Accuracy: 0.8857
Epoch: 29/50, Loss: 0.429, Accuracy: 0.8857
Epoch: 30/50, Loss: 0.418, Accuracy: 0.9
Epoch: 31/50, Loss: 0.4078, Accuracy: 0.9
Epoch: 32/50, Loss: 0.398, Accuracy: 0.9
Epoch: 33/50, Loss: 0.3877, Accuracy: 0.9143
Epoch: 34/50, Loss: 0.3775, Accuracy: 0.9
Epoch: 35/50, Loss: 0.3676, Accuracy: 0.9286
Epoch: 36/50, Loss: 0.357, Accuracy: 0.9143
Epoch: 37/50, Loss: 0.346, Accuracy: 0.9143
Epoch: 38/50, Loss: 0.3384, Accuracy: 0.9286
Epoch: 39/50, Loss: 0.3289, Accuracy: 0.9286
Epoch: 40/50, Loss: 0.3189, Accuracy: 0.9286
Epoch: 41/50, Loss: 0.3112, Accuracy: 0.9143
Epoch: 42/50, Loss: 0.3023, Accuracy: 0.9143
Epoch: 43/50, Loss: 0.2942, Accuracy: 0.9286
Epoch: 44/50, Loss: 0.2851, Accuracy: 0.9286
Epoch: 45/50, Loss: 0.2788, Accuracy: 0.9286
Epoch: 46/50, Loss: 0.2703, Accuracy: 0.9286
Epoch: 47/50, Loss: 0.2645, Accuracy: 0.9286
Epoch: 48/50, Loss: 0.2577, Accuracy: 0.9286
Epoch: 49/50, Loss: 0.2502, Accuracy: 0.9429
Epoch: 50/50, Loss: 0.2455, Accuracy: 0.9286
BCE: 0.227, Accuracy: 0.9333, Fit time: 1.2268 seconds
.
Binary classification. Testing Iris-setosa - Iris-virginica combination
Epoch: 1/50, Loss: 0.7856, Accuracy: 0.5
Epoch: 2/50, Loss: 0.706, Accuracy: 0.5
Epoch: 3/50, Loss: 0.6445, Accuracy: 0.5
Epoch: 4/50, Loss: 0.5955, Accuracy: 0.5857
Epoch: 5/50, Loss: 0.5551, Accuracy: 0.9286
Epoch: 6/50, Loss: 0.5213, Accuracy: 0.9857
Epoch: 7/50, Loss: 0.4914, Accuracy: 1.0
Epoch: 8/50, Loss: 0.4636, Accuracy: 1.0
Epoch: 9/50, Loss: 0.4359, Accuracy: 1.0
Epoch: 10/50, Loss: 0.4092, Accuracy: 1.0
Epoch: 11/50, Loss: 0.3826, Accuracy: 1.0
Epoch: 12/50, Loss: 0.3562, Accuracy: 1.0
Epoch: 13/50, Loss: 0.3305, Accuracy: 1.0
Epoch: 14/50, Loss: 0.3054, Accuracy: 1.0
Epoch: 15/50, Loss: 0.2813, Accuracy: 1.0
Epoch: 16/50, Loss: 0.2582, Accuracy: 1.0
Epoch: 17/50, Loss: 0.2369, Accuracy: 1.0
Epoch: 18/50, Loss: 0.2168, Accuracy: 1.0
Epoch: 19/50, Loss: 0.199, Accuracy: 1.0
Epoch: 20/50, Loss: 0.1825, Accuracy: 1.0
Epoch: 21/50, Loss: 0.1676, Accuracy: 1.0
Epoch: 22/50, Loss: 0.1539, Accuracy: 1.0
Epoch: 23/50, Loss: 0.1415, Accuracy: 1.0
Epoch: 24/50, Loss: 0.1303, Accuracy: 1.0
Epoch: 25/50, Loss: 0.1201, Accuracy: 1.0
Epoch: 26/50, Loss: 0.1108, Accuracy: 1.0
Epoch: 27/50, Loss: 0.1025, Accuracy: 1.0
Epoch: 28/50, Loss: 0.095, Accuracy: 1.0
Epoch: 29/50, Loss: 0.0883, Accuracy: 1.0
Epoch: 30/50, Loss: 0.0822, Accuracy: 1.0
Epoch: 31/50, Loss: 0.0768, Accuracy: 1.0
Epoch: 32/50, Loss: 0.0719, Accuracy: 1.0
Epoch: 33/50, Loss: 0.0674, Accuracy: 1.0
Epoch: 34/50, Loss: 0.0634, Accuracy: 1.0
Epoch: 35/50, Loss: 0.0598, Accuracy: 1.0
Epoch: 36/50, Loss: 0.0564, Accuracy: 1.0
Epoch: 37/50, Loss: 0.0533, Accuracy: 1.0
Epoch: 38/50, Loss: 0.0505, Accuracy: 1.0
Epoch: 39/50, Loss: 0.0479, Accuracy: 1.0
Epoch: 40/50, Loss: 0.0455, Accuracy: 1.0
Epoch: 41/50, Loss: 0.0433, Accuracy: 1.0
Epoch: 42/50, Loss: 0.0413, Accuracy: 1.0
Epoch: 43/50, Loss: 0.0394, Accuracy: 1.0
Epoch: 44/50, Loss: 0.0376, Accuracy: 1.0
Epoch: 45/50, Loss: 0.036, Accuracy: 1.0
Epoch: 46/50, Loss: 0.0345, Accuracy: 1.0
Epoch: 47/50, Loss: 0.0331, Accuracy: 1.0
Epoch: 48/50, Loss: 0.0318, Accuracy: 1.0
Epoch: 49/50, Loss: 0.0306, Accuracy: 1.0
Epoch: 50/50, Loss: 0.0295, Accuracy: 1.0
BCE: 0.0258, Accuracy: 1.0, Fit time: 1.2503 seconds
.
tests/test_multi_classification.py 
Multi classification. Testing on full iris dataset (3 species)
Epoch: 1/50, Loss: 1.0811, Accuracy: 0.4696
Epoch: 2/50, Loss: 0.9458, Accuracy: 0.7652
Epoch: 3/50, Loss: 0.8733, Accuracy: 0.7913
Epoch: 4/50, Loss: 0.8314, Accuracy: 0.8174
Epoch: 5/50, Loss: 0.8123, Accuracy: 0.8435
Epoch: 6/50, Loss: 0.7945, Accuracy: 0.8696
Epoch: 7/50, Loss: 0.781, Accuracy: 0.8957
Epoch: 8/50, Loss: 0.7641, Accuracy: 0.8522
Epoch: 9/50, Loss: 0.7515, Accuracy: 0.913
Epoch: 10/50, Loss: 0.7282, Accuracy: 0.9304
Epoch: 11/50, Loss: 0.7218, Accuracy: 0.8957
Epoch: 12/50, Loss: 0.6988, Accuracy: 0.9304
Epoch: 13/50, Loss: 0.6999, Accuracy: 0.9304
Epoch: 14/50, Loss: 0.6817, Accuracy: 0.9304
Epoch: 15/50, Loss: 0.6708, Accuracy: 0.9304
Epoch: 16/50, Loss: 0.6584, Accuracy: 0.9391
Epoch: 17/50, Loss: 0.6556, Accuracy: 0.9565
Epoch: 18/50, Loss: 0.6536, Accuracy: 0.9565
Epoch: 19/50, Loss: 0.6364, Accuracy: 0.9652
Epoch: 20/50, Loss: 0.6427, Accuracy: 0.9304
Epoch: 21/50, Loss: 0.6343, Accuracy: 0.9478
Epoch: 22/50, Loss: 0.6346, Accuracy: 0.9391
Epoch: 23/50, Loss: 0.6208, Accuracy: 0.9739
Epoch: 24/50, Loss: 0.632, Accuracy: 0.9565
Epoch: 25/50, Loss: 0.6235, Accuracy: 0.9652
Epoch: 26/50, Loss: 0.6245, Accuracy: 0.9391
Epoch: 27/50, Loss: 0.6263, Accuracy: 0.9565
Epoch: 28/50, Loss: 0.6295, Accuracy: 0.9565
Epoch: 29/50, Loss: 0.6208, Accuracy: 0.9565
Epoch: 30/50, Loss: 0.6159, Accuracy: 0.9652
Epoch: 31/50, Loss: 0.6089, Accuracy: 0.9565
Epoch: 32/50, Loss: 0.6178, Accuracy: 0.9391
Epoch: 33/50, Loss: 0.6151, Accuracy: 0.9739
Epoch: 34/50, Loss: 0.6107, Accuracy: 0.9478
Epoch: 35/50, Loss: 0.6171, Accuracy: 0.9565
Epoch: 36/50, Loss: 0.6154, Accuracy: 0.9826
Epoch: 37/50, Loss: 0.5973, Accuracy: 0.9652
Epoch: 38/50, Loss: 0.609, Accuracy: 0.9652
Epoch: 39/50, Loss: 0.611, Accuracy: 0.9652
Epoch: 40/50, Loss: 0.6131, Accuracy: 0.9565
Epoch: 41/50, Loss: 0.6109, Accuracy: 0.9478
Epoch: 42/50, Loss: 0.6118, Accuracy: 0.9652
Epoch: 43/50, Loss: 0.5991, Accuracy: 0.9826
Epoch: 44/50, Loss: 0.6151, Accuracy: 0.9478
Epoch: 45/50, Loss: 0.6053, Accuracy: 0.9652
Epoch: 46/50, Loss: 0.61, Accuracy: 0.9826
Epoch: 47/50, Loss: 0.6005, Accuracy: 0.9739
Epoch: 48/50, Loss: 0.6041, Accuracy: 0.9739
Epoch: 49/50, Loss: 0.606, Accuracy: 0.9565
Epoch: 50/50, Loss: 0.6021, Accuracy: 0.9739
CCE: 0.5783, Accuracy: 1.0, Fit time: 1.7031 seconds
.
Multi classification. Testing on digits dataset
Epoch: 1/20, Loss: 2.1376, Accuracy: 0.4805
Epoch: 2/20, Loss: 1.9571, Accuracy: 0.7755
Epoch: 3/20, Loss: 1.8708, Accuracy: 0.8455
Epoch: 4/20, Loss: 1.8127, Accuracy: 0.876
Epoch: 5/20, Loss: 1.7691, Accuracy: 0.9065
Epoch: 6/20, Loss: 1.7397, Accuracy: 0.9175
Epoch: 7/20, Loss: 1.7161, Accuracy: 0.927
Epoch: 8/20, Loss: 1.6956, Accuracy: 0.933
Epoch: 9/20, Loss: 1.6828, Accuracy: 0.9395
Epoch: 10/20, Loss: 1.6669, Accuracy: 0.947
Epoch: 11/20, Loss: 1.6558, Accuracy: 0.9525
Epoch: 12/20, Loss: 1.6447, Accuracy: 0.956
Epoch: 13/20, Loss: 1.6376, Accuracy: 0.9595
Epoch: 14/20, Loss: 1.627, Accuracy: 0.966
Epoch: 15/20, Loss: 1.6204, Accuracy: 0.9675
Epoch: 16/20, Loss: 1.6139, Accuracy: 0.97
Epoch: 17/20, Loss: 1.6074, Accuracy: 0.972
Epoch: 18/20, Loss: 1.602, Accuracy: 0.974
Epoch: 19/20, Loss: 1.5966, Accuracy: 0.979
Epoch: 20/20, Loss: 1.5904, Accuracy: 0.979
CCE: 1.6406, Accuracy: 0.92, Fit time: 21.6939 seconds
.
tests/test_regression.py 
Regression. Testing func_quadratic
Epoch: 1/50, Loss: 13.5262, R2 score: 0.4417
Epoch: 2/50, Loss: 1.6751, R2 score: 0.9309
Epoch: 3/50, Loss: 1.0717, R2 score: 0.9558
Epoch: 4/50, Loss: 0.9152, R2 score: 0.9622
Epoch: 5/50, Loss: 0.7106, R2 score: 0.9707
Epoch: 6/50, Loss: 0.6319, R2 score: 0.9739
Epoch: 7/50, Loss: 0.506, R2 score: 0.9791
Epoch: 8/50, Loss: 0.4816, R2 score: 0.9801
Epoch: 9/50, Loss: 0.4381, R2 score: 0.9819
Epoch: 10/50, Loss: 0.4551, R2 score: 0.9812
Epoch: 11/50, Loss: 0.4259, R2 score: 0.9824
Epoch: 12/50, Loss: 0.3389, R2 score: 0.986
Epoch: 13/50, Loss: 0.3156, R2 score: 0.987
Epoch: 14/50, Loss: 0.3511, R2 score: 0.9855
Epoch: 15/50, Loss: 0.304, R2 score: 0.9875
Epoch: 16/50, Loss: 0.3121, R2 score: 0.9871
Epoch: 17/50, Loss: 0.2912, R2 score: 0.988
Epoch: 18/50, Loss: 0.2782, R2 score: 0.9885
Epoch: 19/50, Loss: 0.2485, R2 score: 0.9897
Epoch: 20/50, Loss: 0.2531, R2 score: 0.9896
Epoch: 21/50, Loss: 0.2252, R2 score: 0.9907
Epoch: 22/50, Loss: 0.2079, R2 score: 0.9914
Epoch: 23/50, Loss: 0.1449, R2 score: 0.994
Epoch: 24/50, Loss: 0.2041, R2 score: 0.9916
Epoch: 25/50, Loss: 0.1776, R2 score: 0.9927
Epoch: 26/50, Loss: 0.1755, R2 score: 0.9928
Epoch: 27/50, Loss: 0.169, R2 score: 0.993
Epoch: 28/50, Loss: 0.1496, R2 score: 0.9938
Epoch: 29/50, Loss: 0.1343, R2 score: 0.9945
Epoch: 30/50, Loss: 0.1523, R2 score: 0.9937
Epoch: 31/50, Loss: 0.1313, R2 score: 0.9946
Epoch: 32/50, Loss: 0.087, R2 score: 0.9964
Epoch: 33/50, Loss: 0.1179, R2 score: 0.9951
Epoch: 34/50, Loss: 0.0936, R2 score: 0.9961
Epoch: 35/50, Loss: 0.0892, R2 score: 0.9963
Epoch: 36/50, Loss: 0.0852, R2 score: 0.9965
Epoch: 37/50, Loss: 0.0939, R2 score: 0.9961
Epoch: 38/50, Loss: 0.0883, R2 score: 0.9964
Epoch: 39/50, Loss: 0.063, R2 score: 0.9974
Epoch: 40/50, Loss: 0.079, R2 score: 0.9967
Epoch: 41/50, Loss: 0.0675, R2 score: 0.9972
Epoch: 42/50, Loss: 0.0761, R2 score: 0.9969
Epoch: 43/50, Loss: 0.06, R2 score: 0.9975
Epoch: 44/50, Loss: 0.06, R2 score: 0.9975
Epoch: 45/50, Loss: 0.0608, R2 score: 0.9975
Epoch: 46/50, Loss: 0.062, R2 score: 0.9974
Epoch: 47/50, Loss: 0.045, R2 score: 0.9981
Epoch: 48/50, Loss: 0.0477, R2 score: 0.998
Epoch: 49/50, Loss: 0.0453, R2 score: 0.9981
Epoch: 50/50, Loss: 0.0424, R2 score: 0.9982
Test dataset. Loss: 0.0298, R2 score: 0.9988
MSE: 0.0298, r2: 0.9988, Fit time: 1.5387 seconds
.
Regression. Testing func_linear
Epoch: 1/50, Loss: 2.6196, R2 score: 0.3838
Epoch: 2/50, Loss: 0.572, R2 score: 0.8654
Epoch: 3/50, Loss: 0.066, R2 score: 0.9845
Epoch: 4/50, Loss: 0.0232, R2 score: 0.9945
Epoch: 5/50, Loss: 0.0167, R2 score: 0.9961
Epoch: 6/50, Loss: 0.0127, R2 score: 0.997
Epoch: 7/50, Loss: 0.01, R2 score: 0.9976
Epoch: 8/50, Loss: 0.008, R2 score: 0.9981
Epoch: 9/50, Loss: 0.0066, R2 score: 0.9984
Epoch: 10/50, Loss: 0.0056, R2 score: 0.9987
Epoch: 11/50, Loss: 0.0048, R2 score: 0.9989
Epoch: 12/50, Loss: 0.0041, R2 score: 0.999
Epoch: 13/50, Loss: 0.0036, R2 score: 0.9992
Epoch: 14/50, Loss: 0.0032, R2 score: 0.9992
Epoch: 15/50, Loss: 0.0029, R2 score: 0.9993
Epoch: 16/50, Loss: 0.0026, R2 score: 0.9994
Epoch: 17/50, Loss: 0.0023, R2 score: 0.9995
Epoch: 18/50, Loss: 0.0021, R2 score: 0.9995
Epoch: 19/50, Loss: 0.002, R2 score: 0.9995
Epoch: 20/50, Loss: 0.0018, R2 score: 0.9996
Epoch: 21/50, Loss: 0.0017, R2 score: 0.9996
Epoch: 22/50, Loss: 0.0015, R2 score: 0.9996
Epoch: 23/50, Loss: 0.0014, R2 score: 0.9997
Epoch: 24/50, Loss: 0.0013, R2 score: 0.9997
Epoch: 25/50, Loss: 0.0012, R2 score: 0.9997
Epoch: 26/50, Loss: 0.0011, R2 score: 0.9997
Epoch: 27/50, Loss: 0.0011, R2 score: 0.9997
Epoch: 28/50, Loss: 0.001, R2 score: 0.9998
Epoch: 29/50, Loss: 0.0009, R2 score: 0.9998
Epoch: 30/50, Loss: 0.0009, R2 score: 0.9998
Epoch: 31/50, Loss: 0.0008, R2 score: 0.9998
Epoch: 32/50, Loss: 0.0008, R2 score: 0.9998
Epoch: 33/50, Loss: 0.0007, R2 score: 0.9998
Epoch: 34/50, Loss: 0.0007, R2 score: 0.9998
Epoch: 35/50, Loss: 0.0007, R2 score: 0.9998
Epoch: 36/50, Loss: 0.0006, R2 score: 0.9998
Epoch: 37/50, Loss: 0.0006, R2 score: 0.9999
Epoch: 38/50, Loss: 0.0006, R2 score: 0.9999
Epoch: 39/50, Loss: 0.0006, R2 score: 0.9999
Epoch: 40/50, Loss: 0.0005, R2 score: 0.9999
Epoch: 41/50, Loss: 0.0005, R2 score: 0.9999
Epoch: 42/50, Loss: 0.0005, R2 score: 0.9999
Epoch: 43/50, Loss: 0.0005, R2 score: 0.9999
Epoch: 44/50, Loss: 0.0005, R2 score: 0.9999
Epoch: 45/50, Loss: 0.0005, R2 score: 0.9999
Epoch: 46/50, Loss: 0.0004, R2 score: 0.9999
Epoch: 47/50, Loss: 0.0004, R2 score: 0.9999
Epoch: 48/50, Loss: 0.0004, R2 score: 0.9999
Epoch: 49/50, Loss: 0.0004, R2 score: 0.9999
Epoch: 50/50, Loss: 0.0004, R2 score: 0.9999
Test dataset. Loss: 0.0004, R2 score: 0.9999
MSE: 0.0004, r2: 0.9999, Fit time: 1.5058 seconds
.
Regression. Testing func_quadratic_3d
Epoch: 1/20, Loss: 2.1756, R2 score: 0.0952
Epoch: 2/20, Loss: 1.0954, R2 score: 0.5444
Epoch: 3/20, Loss: 0.743, R2 score: 0.691
Epoch: 4/20, Loss: 0.4506, R2 score: 0.8126
Epoch: 5/20, Loss: 0.2413, R2 score: 0.8997
Epoch: 6/20, Loss: 0.1241, R2 score: 0.9484
Epoch: 7/20, Loss: 0.0675, R2 score: 0.9719
Epoch: 8/20, Loss: 0.0431, R2 score: 0.9821
Epoch: 9/20, Loss: 0.0326, R2 score: 0.9865
Epoch: 10/20, Loss: 0.0271, R2 score: 0.9887
Epoch: 11/20, Loss: 0.0238, R2 score: 0.9901
Epoch: 12/20, Loss: 0.0214, R2 score: 0.9911
Epoch: 13/20, Loss: 0.0197, R2 score: 0.9918
Epoch: 14/20, Loss: 0.0181, R2 score: 0.9925
Epoch: 15/20, Loss: 0.0168, R2 score: 0.993
Epoch: 16/20, Loss: 0.0156, R2 score: 0.9935
Epoch: 17/20, Loss: 0.0145, R2 score: 0.994
Epoch: 18/20, Loss: 0.0135, R2 score: 0.9944
Epoch: 19/20, Loss: 0.0128, R2 score: 0.9947
Epoch: 20/20, Loss: 0.0119, R2 score: 0.9951
Test dataset. Loss: 0.0125, R2 score: 0.9951
MSE: 0.0125, r2: 0.9951, Fit time: 26.8006 seconds
.
Regression. Testing func_sin_plus_cos_3d
Epoch: 1/20, Loss: 0.302, R2 score: 0.3754
Epoch: 2/20, Loss: 0.1139, R2 score: 0.7645
Epoch: 3/20, Loss: 0.0604, R2 score: 0.8752
Epoch: 4/20, Loss: 0.0321, R2 score: 0.9337
Epoch: 5/20, Loss: 0.0181, R2 score: 0.9626
Epoch: 6/20, Loss: 0.0158, R2 score: 0.9672
Epoch: 7/20, Loss: 0.0087, R2 score: 0.982
Epoch: 8/20, Loss: 0.0073, R2 score: 0.9848
Epoch: 9/20, Loss: 0.0071, R2 score: 0.9853
Epoch: 10/20, Loss: 0.006, R2 score: 0.9875
Epoch: 11/20, Loss: 0.0047, R2 score: 0.9903
Epoch: 12/20, Loss: 0.005, R2 score: 0.9897
Epoch: 13/20, Loss: 0.0053, R2 score: 0.989
Epoch: 14/20, Loss: 0.0039, R2 score: 0.992
Epoch: 15/20, Loss: 0.0035, R2 score: 0.9928
Epoch: 16/20, Loss: 0.0035, R2 score: 0.9927
Epoch: 17/20, Loss: 0.0031, R2 score: 0.9935
Epoch: 18/20, Loss: 0.0023, R2 score: 0.9952
Epoch: 19/20, Loss: 0.0022, R2 score: 0.9955
Epoch: 20/20, Loss: 0.0022, R2 score: 0.9955
Test dataset. Loss: 0.004, R2 score: 0.9918
MSE: 0.004, r2: 0.9918, Fit time: 27.0536 seconds
.

========================= 9 passed in 87.67s (0:01:27) =========================










Epoch: 1/10, train loss: 2.0412, train Accuracy: 0.565, test loss: 1.8861, test Accuracy: 0.79, epoch time: 94.741 s
Epoch: 2/10, train loss: 1.7296, train Accuracy: 0.91, test loss: 1.7429, test Accuracy: 0.84, epoch time: 95.952 s
Epoch: 3/10, train loss: 1.61, train Accuracy: 0.985, test loss: 1.8214, test Accuracy: 0.79, epoch time: 95.057 s
Epoch: 4/10, train loss: 1.5484, train Accuracy: 0.9925, test loss: 1.8267, test Accuracy: 0.84, epoch time: 95.92 s
Epoch: 5/10, train loss: 1.507, train Accuracy: 0.9975, test loss: 1.7869, test Accuracy: 0.82, epoch time: 94.766 s
Epoch: 6/10, train loss: 1.4937, train Accuracy: 1.0, test loss: 1.7903, test Accuracy: 0.85, epoch time: 94.791 s
Epoch: 7/10, train loss: 1.4918, train Accuracy: 1.0, test loss: 1.7841, test Accuracy: 0.83, epoch time: 94.735 s
Epoch: 8/10, train loss: 1.4882, train Accuracy: 1.0, test loss: 1.7989, test Accuracy: 0.79, epoch time: 94.923 s
Epoch: 9/10, train loss: 1.501, train Accuracy: 1.0, test loss: 1.8038, test Accuracy: 0.83, epoch time: 95.724 s
Epoch: 10/10, train loss: 1.4946, train Accuracy: 1.0, test loss: 1.7927, test Accuracy: 0.81, epoch time: 94.916 s
exec time:  951.5250964164734



platform linux -- Python 3.12.3, pytest-8.3.2, pluggy-1.5.0
rootdir: /home/vlad/Documents/Projects/vladk-neural-network
configfile: pyproject.toml
testpaths: tests
plugins: anyio-4.4.0
collected 9 items

tests/test_binary_classification.py
Binary classification. Testing Iris-setosa - Iris-versicolor combination
BCE: 0.0001, Accuracy: 1.0, Fit time: 2.3385 seconds
.
Binary classification. Testing Iris-versicolor - Iris-virginica combination
BCE: 0.0876, Accuracy: 0.9667, Fit time: 2.4131 seconds
.
Binary classification. Testing Iris-setosa - Iris-virginica combination
BCE: 0.0, Accuracy: 1.0, Fit time: 2.3376 seconds
.
tests/test_multi_classification.py
Multi-class classification. Testing on full Iris dataset (3 species)
CCE: 0.5975, Accuracy: 0.9714, Fit time: 3.4826 seconds
.
Multi-class classification. Testing on Digits dataset
CCE: 1.6147, Accuracy: 0.92, Fit time: 21.3928 seconds
.
tests/test_regression.py
Regression. Testing func_quadratic
MSE: 0.066, R2: 0.9974, Fit time: 2.306 seconds
.
Regression. Testing func_linear
MSE: 0.0, R2: 1.0, Fit time: 2.1879 seconds
.
Regression. Testing func_quadratic_3d
MSE: 0.0015, R2: 0.9994, Fit time: 14.5469 seconds
.
Regression. Testing func_sin_plus_cos_3d
MSE: 0.006, R2: 0.9876, Fit time: 13.7865 seconds
.


return train_dataset[:400], train_dataset[400:500]
layers = [
    Convolutional(LeakyRelu(), filters_num=2, kernel_size=3),
    Convolutional(LeakyRelu(), filters_num=4, kernel_size=3),
    Convolutional(LeakyRelu(), filters_num=8, kernel_size=3),
    Flatten(),
    FullyConnected(128, LeakyRelu()),
    FullyConnected(10, Linear())
]
Epoch: 1/10, train loss: 2.0568, train Accuracy: 0.6025, test loss: 1.7924, test Accuracy: 0.78, epoch time: 162.436s
Epoch: 2/10, train loss: 1.7317, train Accuracy: 0.9375, test loss: 1.8528, test Accuracy: 0.85, epoch time: 160.067s
Epoch: 3/10, train loss: 1.6135, train Accuracy: 0.9975, test loss: 1.8392, test Accuracy: 0.78, epoch time: 159.618s
Epoch: 4/10, train loss: 1.5519, train Accuracy: 0.9975, test loss: 1.8224, test Accuracy: 0.8, epoch time: 161.588s
Epoch: 5/10, train loss: 1.5255, train Accuracy: 1.0, test loss: 1.835, test Accuracy: 0.77, epoch time: 159.78s
Epoch: 6/10, train loss: 1.5098, train Accuracy: 1.0, test loss: 1.7875, test Accuracy: 0.88, epoch time: 158.221s
Epoch: 7/10, train loss: 1.5211, train Accuracy: 1.0, test loss: 1.8637, test Accuracy: 0.8, epoch time: 159.314s
Epoch: 8/10, train loss: 1.5451, train Accuracy: 0.9975, test loss: 1.7577, test Accuracy: 0.81, epoch time: 165.983s
Epoch: 9/10, train loss: 1.5288, train Accuracy: 1.0, test loss: 1.7637, test Accuracy: 0.88, epoch time: 164.638s
Epoch: 10/10, train loss: 1.5167, train Accuracy: 1.0, test loss: 1.8835, test Accuracy: 0.79, epoch time: 164.414s
exec time:  1616.0604257583618



return train_dataset[:800], train_dataset[800:1000]
layers = [
    FullyConnected(256, LeakyRelu()),
    FullyConnected(128, LeakyRelu()),
    FullyConnected(64, LeakyRelu()),
    FullyConnected(10, Linear()),
]
Epoch: 1/30, train loss: 1.9564, train Accuracy: 0.625, test loss: 1.8058, test Accuracy: 0.815, epoch time: 1.105s
Epoch: 2/30, train loss: 1.6986, train Accuracy: 0.8675, test loss: 1.6776, test Accuracy: 0.845, epoch time: 1.162s
Epoch: 3/30, train loss: 1.6143, train Accuracy: 0.9325, test loss: 1.7015, test Accuracy: 0.87, epoch time: 1.103s
Epoch: 4/30, train loss: 1.5668, train Accuracy: 0.9613, test loss: 1.6659, test Accuracy: 0.875, epoch time: 1.128s
Epoch: 5/30, train loss: 1.5308, train Accuracy: 0.9875, test loss: 1.6414, test Accuracy: 0.895, epoch time: 1.051s
Epoch: 6/30, train loss: 1.5088, train Accuracy: 0.99, test loss: 1.6321, test Accuracy: 0.875, epoch time: 1.107s
Epoch: 7/30, train loss: 1.4967, train Accuracy: 0.995, test loss: 1.6244, test Accuracy: 0.87, epoch time: 1.119s
Epoch: 8/30, train loss: 1.4983, train Accuracy: 0.9938, test loss: 1.6564, test Accuracy: 0.895, epoch time: 1.067s
Epoch: 9/30, train loss: 1.4932, train Accuracy: 0.9962, test loss: 1.6491, test Accuracy: 0.89, epoch time: 1.09s
Epoch: 10/30, train loss: 1.4864, train Accuracy: 0.9962, test loss: 1.6088, test Accuracy: 0.88, epoch time: 1.094s
Epoch: 11/30, train loss: 1.4853, train Accuracy: 0.9962, test loss: 1.6272, test Accuracy: 0.87, epoch time: 1.091s
Epoch: 12/30, train loss: 1.4701, train Accuracy: 1.0, test loss: 1.6039, test Accuracy: 0.88, epoch time: 1.064s
Epoch: 13/30, train loss: 1.4733, train Accuracy: 0.9988, test loss: 1.5951, test Accuracy: 0.9, epoch time: 1.087s
Epoch: 14/30, train loss: 1.4917, train Accuracy: 0.995, test loss: 1.6771, test Accuracy: 0.855, epoch time: 1.084s
Epoch: 15/30, train loss: 1.4824, train Accuracy: 0.9975, test loss: 1.5827, test Accuracy: 0.9, epoch time: 1.04s
Epoch: 16/30, train loss: 1.4716, train Accuracy: 1.0, test loss: 1.5812, test Accuracy: 0.895, epoch time: 1.082s
Epoch: 17/30, train loss: 1.469, train Accuracy: 0.9988, test loss: 1.5949, test Accuracy: 0.89, epoch time: 1.083s
Epoch: 18/30, train loss: 1.4681, train Accuracy: 1.0, test loss: 1.5832, test Accuracy: 0.875, epoch time: 1.075s
Epoch: 19/30, train loss: 1.4858, train Accuracy: 0.99, test loss: 1.6011, test Accuracy: 0.895, epoch time: 1.093s
Epoch: 20/30, train loss: 1.4691, train Accuracy: 1.0, test loss: 1.6159, test Accuracy: 0.88, epoch time: 1.15s
Epoch: 21/30, train loss: 1.4701, train Accuracy: 0.9988, test loss: 1.5892, test Accuracy: 0.88, epoch time: 1.092s
Epoch: 22/30, train loss: 1.4775, train Accuracy: 0.995, test loss: 1.6202, test Accuracy: 0.875, epoch time: 1.052s
Epoch: 23/30, train loss: 1.4685, train Accuracy: 1.0, test loss: 1.5899, test Accuracy: 0.89, epoch time: 1.059s
Epoch: 24/30, train loss: 1.4644, train Accuracy: 1.0, test loss: 1.5997, test Accuracy: 0.875, epoch time: 1.074s
Epoch: 25/30, train loss: 1.4809, train Accuracy: 0.995, test loss: 1.6171, test Accuracy: 0.88, epoch time: 1.053s
Epoch: 26/30, train loss: 1.4761, train Accuracy: 0.9962, test loss: 1.6047, test Accuracy: 0.87, epoch time: 1.063s
Epoch: 27/30, train loss: 1.4664, train Accuracy: 1.0, test loss: 1.6139, test Accuracy: 0.865, epoch time: 1.071s
Epoch: 28/30, train loss: 1.4642, train Accuracy: 1.0, test loss: 1.6099, test Accuracy: 0.88, epoch time: 1.11s
Epoch: 29/30, train loss: 1.4642, train Accuracy: 1.0, test loss: 1.6005, test Accuracy: 0.885, epoch time: 1.061s
Epoch: 30/30, train loss: 1.4633, train Accuracy: 1.0, test loss: 1.5951, test Accuracy: 0.885, epoch time: 1.057s
--- 32.57006621360779 seconds ---


layers = [
    Convolutional(LeakyRelu(), filters_num=4, kernel_size=3),
    Convolutional(LeakyRelu(), filters_num=4, kernel_size=3),
    Convolutional(LeakyRelu(), filters_num=4, kernel_size=3),
    Flatten(),
    FullyConnected(128, LeakyRelu()),
    FullyConnected(10, Linear())
]
Epoch: 1/15, train loss: 1.939, train Accuracy: 0.7037, test loss: 1.7799, test Accuracy: 0.875, epoch time: 363.048s
Epoch: 2/15, train loss: 1.6976, train Accuracy: 0.9175, test loss: 1.7182, test Accuracy: 0.87, epoch time: 365.357s
Epoch: 3/15, train loss: 1.6057, train Accuracy: 0.9788, test loss: 1.6302, test Accuracy: 0.925, epoch time: 360.527s
Epoch: 4/15, train loss: 1.5695, train Accuracy: 0.9838, test loss: 1.7497, test Accuracy: 0.88, epoch time: 358.178s
Epoch: 5/15, train loss: 1.5402, train Accuracy: 0.9975, test loss: 1.6659, test Accuracy: 0.915, epoch time: 359.292s
Epoch: 6/15, train loss: 1.5138, train Accuracy: 1.0, test loss: 1.6235, test Accuracy: 0.925, epoch time: 357.846s
Epoch: 7/15, train loss: 1.5101, train Accuracy: 1.0, test loss: 1.6034, test Accuracy: 0.93, epoch time: 358.258s
Epoch: 8/15, train loss: 1.4963, train Accuracy: 1.0, test loss: 1.5893, test Accuracy: 0.92, epoch time: 359.548s
Epoch: 9/15, train loss: 1.4891, train Accuracy: 1.0, test loss: 1.7042, test Accuracy: 0.905, epoch time: 362.712s
Epoch: 10/15, train loss: 1.4958, train Accuracy: 1.0, test loss: 1.6438, test Accuracy: 0.92, epoch time: 361.976s
Epoch: 11/15, train loss: 1.4908, train Accuracy: 1.0, test loss: 1.6318, test Accuracy: 0.895, epoch time: 361.59s
Epoch: 12/15, train loss: 1.4874, train Accuracy: 1.0, test loss: 1.6377, test Accuracy: 0.915, epoch time: 360.399s
Epoch: 13/15, train loss: 1.4879, train Accuracy: 1.0, test loss: 1.6226, test Accuracy: 0.94, epoch time: 359.11s
Epoch: 14/15, train loss: 1.485, train Accuracy: 1.0, test loss: 1.6146, test Accuracy: 0.915, epoch time: 364.369s
Epoch: 15/15, train loss: 1.4799, train Accuracy: 1.0, test loss: 1.6611, test Accuracy: 0.93, epoch time: 362.149s
exec time:  5414.361704349518


layer_error
tensor([[[ 7.5428e-04, -1.9525e-04, -1.3240e-03,  2.2925e-02],
         [ 2.1448e-03, -2.1429e-03,  8.1483e-04,  8.4673e-03],
         [ 7.9308e-05, -3.0675e-03,  3.5140e-03, -1.6897e-02],
         [-2.2638e-01,  1.6438e-01, -1.8934e-01, -1.8707e-03]],

        [[ 6.8957e-02, -8.7830e-02, -1.5290e-02, -1.4721e-04],
         [ 1.3041e-02, -4.0039e-04,  2.9343e-03,  1.7664e-02],
         [-2.0738e-01, -1.7901e-04, -3.6541e-03, -1.2346e-01],
         [ 1.3839e-03,  1.1451e-03,  2.3403e-03,  1.8894e-01]]])


forward single core
Forward time:  0.06352400779724121
Forward time:  0.1104423999786377
Forward time:  0.18530988693237305

Forward time:  0.06598258018493652
Forward time:  0.11107468605041504
Forward time:  0.18769335746765137

Forward time:  0.06174159049987793
self.a[0]
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2717e-01,
          4.0219e-02, -2.3236e-04,  2.3859e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          4.3600e-02,  4.1662e-02, -2.1981e-04,  1.4565e-02,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0900e-02,  2.5272e-01,
          1.7032e-01,  2.3872e-01,  2.1286e-01,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7855e-02,
          1.6288e-01,  1.2766e-01,  6.8591e-02,  1.4232e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  1.8167e-03,  1.1279e-01,  4.8129e-02,
         -1.2544e-03,  1.3342e-01,  1.6876e-01,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.4321e-02,
          1.7961e-01,  5.1010e-02,  2.0595e-01,  2.0364e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  6.3863e-02,  1.9274e-01,  1.2906e-02,
         -8.9223e-04,  1.0568e-01,  4.8241e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7250e-02,  1.5315e-01,
         -1.1584e-05, -1.3809e-03,  8.8352e-02,  4.1196e-02,  0.0000e+00,
          0.0000e+00,  1.8772e-02,  1.7236e-01,  8.4113e-02, -3.9078e-04,
          1.7320e-02,  1.3321e-01, -4.2065e-05,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  1.9378e-02,  1.6099e-01,  1.1219e-01,
         -3.7868e-04,  4.9846e-02,  1.6558e-01,  4.4804e-02,  0.0000e+00,
          4.8444e-03,  1.1895e-01,  1.2738e-01, -1.2749e-04, -8.8952e-04,
          5.1228e-02,  3.6061e-02, -7.4654e-05,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  9.1601e-02,  2.0416e-01,  3.9240e-02,
         -3.1554e-04,  1.1966e-02,  3.1401e-02, -2.0841e-04,  0.0000e+00,
          5.1210e-02,  1.9101e-01,  3.7000e-02, -4.1415e-04, -1.1964e-05,
          8.4605e-02, -1.0887e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  1.5167e-01,  8.9318e-03, -3.3090e-04,
         -1.7116e-04,  7.9929e-02,  5.9826e-03,  8.5086e-03,  1.9970e-02,
          1.7126e-01,  6.6202e-02, -6.7818e-04, -6.6460e-04,  3.9016e-02,
         -2.0007e-04, -7.1543e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  4.4217e-02, -2.9920e-04,  5.1060e-02,
          2.0605e-01,  2.7236e-01,  1.9732e-01,  1.4778e-01,  1.3247e-01,
          1.9460e-01, -2.4695e-05, -5.7735e-04,  1.8313e-02,  1.8211e-03,
         -1.8041e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -1.2266e-03, -2.1061e-03, -8.6119e-04,
          1.9850e-01,  3.7629e-01,  4.2380e-01,  5.4257e-01,  4.8012e-01,
          2.9603e-01, -3.9939e-04, -1.4530e-04,  2.8193e-02, -3.3905e-04,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -8.2072e-04, -2.6241e-03, -3.6059e-03,
         -3.4992e-03, -2.4430e-03, -1.5360e-03, -2.3479e-04, -8.7804e-04,
         -4.3883e-04, -6.2650e-04,  7.6560e-02, -3.1390e-04,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5716e-04, -1.0435e-03,
         -2.2358e-03, -2.9590e-03, -9.5020e-04, -9.1918e-04, -9.7484e-04,
         -9.7103e-04,  5.5149e-02,  2.8209e-02, -5.9101e-05,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          2.4222e-03,  1.1358e-01,  1.4865e-01, -1.5467e-05, -6.5995e-04,
         -8.6402e-05,  2.7592e-02, -3.2972e-04,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.6333e-03,
          9.7050e-02,  1.9012e-01,  5.8823e-02, -3.5955e-04, -5.0392e-04,
          5.1469e-02, -4.6226e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1721e-02,
          2.0094e-01,  9.1892e-02,  1.3487e-02, -7.9387e-04,  4.8009e-02,
         -2.7134e-04, -1.5553e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2094e-02,  1.6733e-01,
          9.2133e-02, -2.2600e-04, -7.0128e-04,  2.3389e-02,  1.3328e-03,
         -1.3064e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  6.6611e-03,  1.3381e-01,  1.4105e-01,
         -1.0948e-04, -7.7166e-04, -2.1134e-04, -1.2738e-04, -3.0795e-04,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  5.1490e-02,  1.7700e-01,  8.3662e-03,
         -9.3135e-04, -2.4589e-04,  1.3811e-02, -3.5636e-04,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  4.8444e-03,  1.6017e-01,  6.0449e-02, -6.1923e-04,
         -5.8666e-04,  7.6501e-02, -3.1487e-04, -9.3317e-06,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  1.0638e-02,  1.7186e-01, -1.8494e-04, -5.3188e-04,
          6.5871e-02,  8.1131e-02, -6.2211e-05,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -1.5170e-05, -9.0845e-04, -2.0934e-03, -7.8778e-04,
          7.4480e-02, -3.0708e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -9.6042e-05, -1.9459e-03, -3.5577e-03, -2.6411e-03,
         -1.0001e-03, -1.2442e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00]])
Forward time:  0.11795353889465332
self.a[0]
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  1.9448e-02, -1.3394e-04,  1.6136e-02,
         -1.4403e-04, -3.0783e-04,  5.2576e-02,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  6.6680e-03, -1.7184e-05,
          3.3569e-03,  1.9148e-03, -2.9179e-04,  3.2096e-02,  0.0000e+00,
          0.0000e+00,  1.6670e-03,  6.7456e-02, -4.1188e-04,  8.9892e-02,
          5.7028e-02, -2.8801e-04,  4.0151e-02,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  4.2601e-03,  3.0311e-02, -1.0512e-04,
          2.6662e-02,  3.4798e-02, -3.3606e-04,  5.1770e-02,  0.0000e+00,
          2.7783e-04,  1.9556e-02,  1.3003e-02, -3.6800e-04,  7.0696e-02,
          7.1244e-02, -1.8654e-04,  1.0146e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  2.1178e-02,  2.7758e-02, -2.4116e-04,
          6.0900e-02,  7.6189e-02, -2.3254e-04,  5.9739e-02,  0.0000e+00,
          1.0207e-02,  4.1603e-02, -1.1557e-03, -8.5520e-05,  5.5991e-02,
         -1.5406e-03,  1.4917e-02,  8.9662e-02,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  4.1675e-03,  3.4479e-02, -4.9356e-04, -2.9308e-05,
          8.2434e-02, -6.6954e-04, -3.2382e-04,  1.2540e-01,  2.8709e-03,
          3.7919e-02, -1.6322e-04, -3.6447e-05, -1.3064e-05,  1.4402e-02,
         -9.9048e-04,  8.2713e-02,  4.9932e-02,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          2.9635e-03,  2.7631e-02, -6.9050e-05, -6.6057e-04, -3.4294e-04,
          4.6258e-02, -1.4014e-03,  1.0149e-01,  2.8385e-02,  2.1843e-02,
          1.7339e-02, -7.9575e-04,  8.4963e-03,  5.1337e-02, -6.9329e-04,
         -2.5871e-04,  1.2232e-01,  2.2015e-02,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          1.8707e-02,  3.8910e-02, -6.9797e-04,  1.7026e-02,  9.1902e-02,
         -9.4522e-04, -7.1750e-05,  7.7760e-02,  6.0233e-02,  4.2060e-02,
         -6.8824e-04,  1.5627e-02,  5.2962e-04, -1.5874e-04, -6.3030e-04,
          7.4411e-02,  6.8874e-02,  6.3377e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          4.0705e-02, -4.3294e-04,  8.7893e-03,  3.9081e-02, -2.1345e-04,
         -8.2222e-04,  1.6302e-02,  1.2080e-01,  4.0924e-02, -1.7906e-04,
         -3.2841e-04, -1.3381e-04,  3.2995e-02, -4.3431e-04, -1.6736e-04,
          1.1756e-01,  2.2878e-02,  1.7544e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          1.2980e-02, -1.1591e-03,  4.6414e-02,  2.7363e-02,  1.7961e-02,
          3.4168e-02,  1.6246e-01,  4.4308e-02,  6.2808e-02, -1.1994e-03,
          2.4454e-02,  6.3542e-02, -3.6735e-04, -6.7487e-04,  1.0794e-01,
          4.7241e-02,  7.6849e-03,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -3.2631e-04, -2.8178e-04, -5.7776e-06,  5.1969e-02,  5.0680e-02,
          9.1135e-02,  1.3176e-01,  9.2688e-02,  8.2152e-02, -1.3204e-04,
          7.8151e-02, -7.4141e-04, -4.4613e-04,  7.3638e-02,  8.8207e-02,
          1.1954e-02,  1.6813e-03,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -1.2318e-04, -3.5642e-05, -9.3920e-05, -5.4906e-04, -1.4043e-03,
         -5.2258e-05,  1.2030e-01,  8.1644e-03,  3.7544e-02,  9.9969e-03,
          6.1229e-02, -2.3127e-04,  2.3573e-02,  1.2789e-01,  1.0794e-02,
          4.2398e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          5.7220e-03, -2.1177e-04, -2.5094e-04, -7.1029e-04, -5.7710e-04,
         -9.5490e-04, -1.7128e-03, -1.1766e-03, -9.8117e-04, -1.2676e-04,
          1.4999e-03, -4.4621e-04,  1.4483e-01,  2.0606e-02,  7.9680e-03,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          4.7551e-03,  8.1386e-03,  2.3968e-02,  3.0003e-02,  1.8140e-02,
          8.4319e-02, -5.1004e-05,  1.6792e-02,  4.2339e-02, -1.8983e-04,
         -5.8791e-04,  8.8136e-02,  7.1404e-02,  1.1612e-02,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  9.1056e-04,  5.3049e-03,  2.5947e-02,  6.8357e-02,
         -1.3603e-04,  1.5070e-02,  5.3581e-02, -2.5676e-04, -3.9307e-04,
          1.6322e-02,  1.2101e-01,  1.1201e-02,  1.3889e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  1.0320e-02,  4.0767e-02, -2.1873e-04,
         -2.4565e-04,  2.6398e-02, -1.1196e-04,  3.5696e-03, -4.6947e-04,
          1.6014e-01,  1.4432e-02,  7.7487e-03,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  4.9084e-03,  3.3718e-02, -5.7235e-05, -5.0851e-04,
         -4.5148e-06, -7.6340e-05,  2.9610e-02, -8.5378e-04,  1.3779e-01,
          3.3990e-02,  1.1978e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          1.0187e-03,  2.7009e-02,  1.0957e-02, -7.3123e-04, -1.2527e-04,
          1.9300e-02, -6.4409e-05, -9.0252e-04,  8.8378e-02,  7.6273e-02,
          1.4504e-02,  3.6550e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          9.4895e-03,  3.9514e-02, -6.8865e-04,  1.2360e-02,  1.8741e-02,
         -4.0763e-04, -4.5876e-04,  5.6339e-02,  1.0900e-01,  1.1337e-02,
          3.0702e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.4089e-04,
          3.4465e-02, -2.7877e-04, -2.9894e-04, -1.8016e-04,  5.4224e-02,
         -6.7833e-04,  3.4387e-02,  1.2917e-01,  1.2928e-02,  7.2370e-03,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.8013e-03,
          4.5817e-02, -1.0638e-03,  4.9493e-02,  3.2880e-02, -4.1912e-04,
         -2.5154e-04,  1.5015e-01,  2.2694e-02,  9.0434e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.4810e-04,
         -1.9305e-04, -8.3201e-04, -1.4927e-04,  5.9382e-02, -7.8189e-04,
          4.3681e-02,  8.4085e-02,  1.1117e-02,  2.1930e-04,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.3690e-05,
         -5.4588e-04, -3.5337e-05, -2.2497e-04, -8.0468e-04, -4.1526e-04,
          1.2534e-01,  2.8733e-02,  1.4620e-03,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2526e-05,
          5.0267e-03, -2.9592e-04, -4.1757e-05,  4.3714e-02,  7.7562e-02,
          2.9869e-02,  9.6368e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])
Forward time:  0.18132495880126953
self.a[0]
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  3.2476e-04, -2.3155e-05, -1.4006e-07,  6.8807e-04,
         -9.8982e-06,  4.7922e-03, -2.4401e-05,  5.4522e-03,  8.1189e-05,
          3.2475e-03, -1.7453e-04,  1.9871e-02, -5.7595e-05, -1.1498e-04,
          2.3314e-02,  1.0409e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          2.0748e-04, -1.0243e-07, -8.3480e-05,  6.9420e-03,  1.6393e-03,
         -1.2292e-04,  1.8503e-02,  2.4335e-03,  8.0998e-03,  9.0325e-04,
         -2.4224e-05,  2.9262e-03,  4.5757e-02,  5.2620e-04, -1.1443e-04,
          9.7278e-03,  1.8014e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          1.1732e-03, -2.9636e-05, -1.4187e-05,  2.9315e-02, -6.2409e-05,
         -1.2827e-04,  1.4772e-02,  2.3541e-02, -1.4804e-06, -6.5311e-06,
         -8.6740e-05, -4.9213e-05,  1.8462e-04, -5.7966e-05,  4.4138e-02,
          5.2304e-02, -1.4194e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0297e-04,
          1.0469e-03, -7.0873e-05,  1.0431e-02,  1.9926e-02, -1.8480e-04,
          1.8135e-02,  4.0279e-02,  3.8567e-03,  1.7945e-03, -7.0380e-05,
          6.1347e-03,  2.2408e-02,  1.4110e-03, -4.5747e-04,  2.8645e-02,
          1.2694e-02, -7.8638e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4434e-04,  3.0121e-04,
         -6.3378e-05,  4.2903e-04,  1.6163e-02, -6.5335e-05, -1.9629e-04,
          4.8786e-02,  3.2624e-02, -9.6150e-05,  1.2904e-03, -2.7756e-06,
          1.7189e-02, -3.4389e-06, -1.2159e-04,  3.0720e-02,  3.1958e-02,
          8.3533e-03, -2.3551e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0121e-03, -2.1227e-05,
         -3.9182e-05,  1.2142e-02,  5.9476e-03,  2.5031e-03, -1.8827e-05,
          2.3074e-02, -7.0820e-05,  2.9900e-03, -1.2280e-04,  7.2066e-03,
          1.2441e-02,  7.8226e-03, -1.0195e-04,  3.2992e-02,  2.0228e-02,
         -1.0036e-04,  5.6124e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7036e-03, -6.8323e-05,
          1.0087e-02,  2.0036e-02,  5.7010e-03, -2.2798e-04,  4.4863e-02,
          2.0461e-02,  1.5747e-03, -9.0912e-05,  1.2240e-02,  2.8874e-02,
          3.8654e-05, -2.3393e-04,  3.5331e-02,  2.3113e-02, -4.0731e-05,
         -5.4627e-05,  2.2885e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7552e-03, -3.7683e-05,
          1.3379e-02, -2.9921e-05, -2.9662e-05,  1.0477e-02, -6.8524e-05,
          9.9989e-04, -8.0011e-05,  1.5604e-02,  2.2920e-02, -6.1054e-06,
         -2.7963e-05,  8.2808e-03,  3.2144e-02,  1.3438e-02, -1.5135e-04,
          2.8504e-03, -2.7784e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -5.3610e-06, -7.9035e-05,
          2.7534e-02,  1.4674e-02, -8.0931e-05, -1.2768e-06,  3.8858e-02,
          5.0574e-03,  3.0624e-02,  7.1653e-03,  4.1746e-02,  1.1029e-02,
         -1.2624e-04,  4.4658e-02,  1.0709e-02, -9.9199e-05, -3.6969e-05,
         -2.0027e-05,  1.0195e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.8681e-05, -4.5494e-05,
         -3.8812e-05, -1.1878e-04,  1.9076e-02,  4.5562e-02,  5.3924e-02,
          1.4856e-02,  3.0483e-02, -1.0647e-04, -1.0567e-04, -3.0335e-04,
          4.4510e-02,  9.1779e-03,  2.9138e-03, -2.4744e-05,  4.3852e-03,
         -8.4009e-06,  6.8268e-04],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.0484e-04, -3.7474e-06,
          6.4221e-03, -3.8630e-05, -1.8713e-05, -9.6831e-05, -4.5777e-04,
         -2.0662e-04,  1.3703e-02,  1.3737e-02, -6.5408e-05,  2.5846e-03,
          7.5661e-03,  2.2836e-02, -9.7554e-05,  1.9443e-03, -7.0373e-05,
          2.0584e-03,  9.6107e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -5.8265e-06,  3.5122e-03,
          7.0656e-03,  1.1214e-02, -8.4223e-05, -6.3677e-05,  1.7045e-03,
         -2.5443e-05, -1.6356e-04, -4.8375e-04, -3.1473e-04,  3.3630e-02,
          2.9652e-02, -6.5960e-05, -2.4554e-05, -1.0776e-04,  3.9022e-03,
          2.4236e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -4.9429e-06, -2.3409e-05,
         -1.6916e-05,  5.7723e-03,  1.8342e-02,  1.8448e-02,  1.9315e-02,
          2.7612e-02,  2.6347e-02,  2.5435e-02,  6.1825e-02,  2.8886e-02,
         -7.4405e-05, -1.0072e-04, -3.5484e-05,  4.3999e-03,  4.5546e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -6.9734e-06, -1.5541e-05,
          1.0140e-03, -2.1582e-05, -1.2997e-04,  6.5551e-03,  1.4791e-02,
          1.7632e-02, -1.3304e-04,  1.8620e-02,  1.4707e-02,  7.3450e-03,
         -1.4581e-04,  3.6605e-03, -2.6629e-05,  7.8925e-04,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0552e-04,
         -8.3325e-06, -7.8394e-05,  7.3471e-03,  1.2735e-02, -1.2822e-05,
         -2.3025e-04,  7.2471e-03,  1.4615e-02,  1.9767e-02, -6.8949e-05,
          3.2320e-03, -9.8082e-05,  2.8073e-03,  7.9393e-05,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9615e-05,  1.0722e-03,
         -5.1876e-05,  1.8707e-03,  7.7341e-03,  5.1293e-03, -8.8484e-05,
          8.8636e-03,  2.0506e-02,  1.6639e-02, -3.7179e-05, -4.9405e-05,
         -1.1576e-04,  5.4232e-03,  4.4293e-04,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9869e-04,  1.2188e-04,
         -6.9384e-05,  1.0481e-02,  1.3217e-02, -3.6304e-05, -7.9006e-05,
          3.1144e-02,  1.4128e-02, -2.7015e-05, -8.4053e-05, -1.5721e-05,
          3.8132e-03,  7.1762e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.6084e-05,  1.7500e-03, -6.6773e-05,
          3.7587e-03,  2.5550e-02,  1.3651e-03, -1.7565e-04,  3.6599e-02,
          1.3933e-02, -4.3959e-06, -4.6934e-05,  3.1448e-03, -3.8718e-05,
          1.3797e-03,  2.0893e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.6057e-04,  3.0006e-03, -1.1184e-04,
          1.4239e-02, -3.4517e-05,  1.2879e-03, -5.9317e-05,  1.8612e-02,
          2.0364e-02, -1.8018e-05,  2.2800e-04, -9.9028e-05,  3.3642e-03,
          1.7550e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.3128e-04,  3.6436e-03,  5.2701e-03,
          3.3342e-02,  6.6454e-03, -6.5492e-05,  3.0015e-02,  2.0732e-02,
          2.9801e-03, -4.4991e-05, -1.1458e-04,  4.8447e-03,  4.1368e-04,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2069e-06, -4.8512e-05, -1.3367e-04,
          2.0196e-03,  2.3727e-03, -5.0267e-05,  1.2572e-02,  1.4152e-02,
         -1.3111e-04, -4.7359e-05,  4.3129e-03,  5.3671e-04,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5901e-06, -2.0900e-05, -5.3669e-05,
          1.4086e-03, -6.4758e-05, -1.1111e-05,  1.9125e-03, -1.0002e-04,
          3.7343e-04, -1.0918e-05,  8.5120e-04,  1.2536e-05,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00]])












self.a[-1]
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0003e-03,
         -3.2705e-03,  6.2766e-02,  2.1585e-01,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -3.4296e-04, -1.3406e-03, -3.2207e-04,  1.3177e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -8.5741e-05, -3.8317e-04,
         -5.4037e-04,  7.2726e-02,  7.3017e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.1912e-04,
         -1.2913e-03, -1.8322e-03,  9.3389e-02,  1.5649e-01,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -1.4290e-05, -7.7021e-04, -8.0998e-04,
          9.6944e-02,  1.2312e-01,  3.6366e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.3496e-04,
         -6.1524e-04,  9.5109e-02,  1.4885e-01, -2.8141e-04,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -4.7582e-04, -1.4409e-03,  1.2418e-01,
          2.5721e-02,  9.3191e-02,  5.1940e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -2.1435e-04, -9.0530e-04,
         -3.9197e-04,  7.2909e-02,  2.5173e-02,  1.2227e-01,  0.0000e+00,
          0.0000e+00, -1.4767e-04, -8.8792e-04, -2.8292e-04,  1.6157e-01,
          1.0470e-01, -6.6357e-05,  3.5314e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -1.5243e-04, -1.3184e-03, -1.4528e-03,
          2.8470e-01,  8.4021e-02,  6.6894e-02, -1.1841e-04,  0.0000e+00,
         -3.8107e-05, -7.7392e-04, -1.2734e-03,  1.6472e-01,  9.2872e-02,
          4.6138e-02,  3.8000e-02,  1.4578e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -4.3743e-04, -9.0891e-04,  1.8060e-01,
          4.8169e-02,  6.8450e-02, -8.0295e-05,  4.0697e-02,  0.0000e+00,
         -3.3205e-04, -1.0274e-03,  6.1249e-02,  1.3088e-01,  1.6539e-01,
         -7.1926e-04,  4.7231e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -1.6475e-04, -9.5196e-05,  5.9703e-04,
         -1.0905e-05, -1.6561e-04,  1.3656e-01,  5.1135e-03, -2.1805e-04,
         -1.0065e-03, -8.0401e-04,  2.2487e-01,  1.8259e-01, -8.3887e-04,
          5.0391e-02,  1.3971e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  1.0655e-01,  1.0270e-01, -1.6649e-03,
         -1.4870e-03, -9.4266e-04, -1.3107e-03, -7.8873e-04, -8.7422e-04,
          1.0392e-01,  3.1004e-01,  1.1352e-01,  2.1651e-02, -1.7079e-04,
          3.5231e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  1.3261e-01,  1.3122e-01,  1.7802e-01,
         -2.0655e-04,  5.1680e-02,  1.3536e-01,  1.3407e-01, -7.3701e-04,
          7.2306e-02,  5.2913e-02,  1.5509e-01, -9.4211e-04,  6.6209e-02,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  4.1766e-02,  2.1146e-02,  1.0853e-02,
          1.8333e-01,  1.7954e-01,  5.3232e-02, -1.1916e-03, -6.0707e-04,
          1.7864e-01,  1.7419e-01, -8.7648e-04,  8.2751e-02,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  7.9978e-03,  2.9007e-02,
         -1.8694e-04, -2.4482e-04, -1.0512e-03, -7.5541e-04,  1.9198e-01,
          1.9163e-01,  8.2807e-03,  1.9855e-02,  1.1541e-02,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -1.9054e-05, -8.7889e-04, -1.7738e-03,  2.2698e-01,  1.0502e-01,
          1.5307e-01, -1.0846e-03,  6.4387e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.8580e-05,
         -8.1243e-04, -2.0151e-03,  1.9958e-01,  1.1113e-01,  2.1427e-01,
         -1.6932e-03,  9.5915e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.3243e-04,
         -1.4648e-03,  7.5152e-02,  1.4736e-01,  2.0026e-01, -1.3312e-03,
          8.8635e-02,  3.0371e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -2.5246e-04, -1.2182e-03,
         -3.0367e-04,  1.9714e-01,  2.0420e-01, -5.2892e-04,  1.0265e-02,
          2.5512e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -5.2397e-05, -7.3842e-04, -1.0589e-03,
          1.9628e-01,  1.5711e-01,  3.7136e-02, -6.2566e-04,  6.0135e-02,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -3.0771e-04, -8.3439e-04,  5.3911e-02,
          1.3247e-01,  1.4155e-01, -1.0124e-03,  7.2977e-02,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -3.8107e-05, -7.2674e-04, -9.2058e-04,  1.6456e-01,
          8.8078e-02, -2.2806e-04,  7.7004e-02,  1.8223e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -1.2899e-05,  3.7243e-02,  4.8759e-02,  8.7100e-03,
          9.2386e-02,  2.3738e-02,  1.2149e-02,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  8.9856e-03,  2.0834e-01,  2.3405e-01,  1.1824e-02,
         -1.7640e-03,  5.1162e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  4.8875e-03,  8.4300e-02, -1.0048e-03, -7.8707e-04,
          1.0110e-01,  2.4297e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00]])
self.a[-1]
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -7.1688e-05,  1.6441e-02,  2.0837e-02,
          1.6161e-02,  4.3255e-02,  5.1854e-02,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4579e-05,  2.1944e-03,
          7.9973e-03,  5.2354e-03,  1.5880e-02,  3.1655e-02,  0.0000e+00,
          0.0000e+00, -6.1447e-06, -1.1292e-05,  3.2832e-02,  2.1986e-02,
         -2.5124e-04,  9.2986e-02,  9.1059e-02,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -1.5703e-05, -2.1715e-05,  1.7740e-02,
          2.1018e-02, -2.0136e-04,  6.8215e-02,  8.2474e-02,  0.0000e+00,
         -1.0241e-06, -5.1423e-05,  2.5375e-03,  5.2386e-02,  8.7435e-02,
          1.5590e-01,  2.1353e-01,  1.1593e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -2.6927e-05,  5.2797e-03,  5.4515e-02,
          3.1414e-02,  7.4094e-02,  1.6565e-01,  1.0209e-01,  0.0000e+00,
         -3.4290e-05,  3.8279e-03,  4.8659e-02,  1.0407e-01,  2.3859e-02,
          1.0810e-01,  1.6462e-01,  4.4430e-02,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -1.5362e-05, -3.6554e-05,  3.9555e-02,  5.7285e-02,
          8.3239e-02,  1.5093e-01,  2.0034e-01,  7.1805e-02, -1.0582e-05,
         -2.1836e-05,  1.9734e-02,  3.0759e-02,  6.1625e-02,  7.8601e-02,
          1.3591e-01,  1.2337e-01,  6.5586e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -1.0924e-05, -4.4887e-05,  1.6828e-02,  4.1635e-02,  6.8000e-02,
          5.0948e-02,  1.0107e-01,  1.2064e-01,  5.8680e-03, -4.4319e-05,
          7.7904e-03,  4.9684e-02,  9.7471e-02,  7.2064e-02,  1.3050e-01,
          1.4146e-01,  6.6228e-02, -5.4830e-06,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -3.3381e-05,  7.1533e-03,  5.2318e-02,  6.5621e-02,  1.0483e-01,
          1.3830e-01,  1.8009e-01,  9.5243e-02,  1.2979e-02,  1.3654e-03,
          4.4295e-02,  5.5154e-02,  6.3676e-02,  9.1759e-02,  1.0716e-01,
          9.9344e-02,  3.6527e-03,  2.7141e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -4.6140e-05,  3.2639e-02,  3.4770e-02,  9.3791e-02,  6.7192e-02,
          8.0334e-02,  9.0334e-02,  2.3183e-02, -5.0926e-05,  2.3113e-02,
          3.9909e-02,  7.7237e-02,  9.3283e-02,  1.3288e-01,  1.0255e-01,
          4.5714e-02,  2.2524e-03,  8.4955e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -3.6752e-05,  2.8643e-02,  1.1783e-01, -1.7408e-04, -8.1169e-05,
          3.9324e-02,  1.2227e-01,  8.5675e-02,  5.8715e-02,  8.6536e-02,
          1.2658e-01,  1.1782e-01,  1.4564e-01,  8.3257e-02,  5.5554e-02,
         -3.6306e-05,  3.3813e-03,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -2.4331e-04,  3.1188e-02,  4.4609e-02,  1.7060e-02,  3.8531e-02,
          1.2422e-01,  1.0383e-01,  1.0076e-01,  5.1692e-02,  6.2234e-02,
          1.4635e-01,  1.3825e-01,  9.4053e-02,  5.3899e-02,  5.3746e-03,
          5.6958e-03,  8.1415e-04,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          2.5977e-03,  5.9335e-02,  5.7536e-02,  8.8573e-02,  1.3128e-01,
          1.8453e-01,  2.1662e-01,  2.1547e-01,  2.1681e-01,  2.0683e-01,
          1.4183e-01,  9.8742e-02,  7.3182e-02,  2.3247e-02,  5.7159e-03,
          2.0531e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          8.1219e-03,  1.2560e-02,  2.4631e-02, -2.7799e-04,  2.0543e-02,
          5.9918e-02,  9.6225e-03,  2.0457e-03, -5.2930e-04,  2.2854e-02,
          8.3474e-02,  7.8897e-02,  6.7083e-02,  7.1907e-03,  3.8584e-03,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          4.6449e-03, -4.2188e-05, -2.8752e-04, -2.5692e-04, -3.6824e-04,
         -4.8244e-04, -1.0668e-04,  1.5812e-02,  9.2669e-02,  1.4528e-01,
          1.0490e-01,  8.3545e-02,  9.0239e-03,  5.3419e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  8.8945e-04,  2.0014e-03, -1.2721e-04, -1.3869e-04,
          1.7783e-02,  2.6310e-02,  1.2231e-01,  1.2789e-01,  1.1274e-01,
          7.7366e-02,  1.6992e-02,  4.8392e-04,  6.7256e-04,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -3.1371e-05,  3.7645e-03,  2.7002e-02,
          3.4132e-02,  1.2046e-01,  1.1718e-01,  1.3735e-01,  7.6325e-02,
          5.4264e-02,  6.2680e-03,  3.7522e-03,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -1.8093e-05, -1.1536e-05,  1.7935e-02,  4.4260e-02,
          1.0566e-01,  1.2289e-01,  1.4961e-01,  6.6283e-02,  6.7675e-02,
          8.3883e-03,  5.7261e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -3.7551e-06, -3.8255e-05,  9.9627e-03,  5.4628e-02,  7.4265e-02,
          1.0492e-01,  1.4463e-01,  7.4959e-02,  5.5215e-02,  6.5319e-03,
          6.5687e-03,  1.7699e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -2.2751e-05,  4.0959e-04,  5.0688e-02,  5.4475e-02,  8.6320e-02,
          1.3430e-01,  9.1830e-02,  3.1228e-02,  6.8904e-03,  5.2333e-03,
          1.4867e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7310e-06,
         -5.2869e-05,  2.1693e-02,  3.9405e-02,  6.9348e-02,  7.7632e-02,
          8.8514e-02,  6.3388e-02,  2.5346e-02,  8.9160e-03,  3.5044e-03,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4325e-06,
         -3.5253e-05,  5.4315e-02,  1.0517e-01,  5.6362e-02,  9.8944e-02,
          8.9294e-02,  7.8385e-02,  9.5840e-03,  4.3348e-03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.5904e-06,
         -1.5237e-04,  2.8877e-02,  1.1395e-01,  8.3160e-02,  6.1327e-02,
          1.1339e-01,  3.0561e-02,  5.1873e-03,  1.0619e-04,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0102e-05,
         -1.0253e-04,  8.2023e-02,  6.7980e-02,  3.2319e-02, -1.5271e-05,
          4.2105e-02, -4.0186e-05,  7.0795e-04,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.7010e-05,
          6.3341e-03,  2.6257e-02, -3.3645e-04, -9.5506e-04, -2.7264e-04,
         -7.3347e-05,  4.0752e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])
self.a[-1]
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -3.5439e-06,  5.6394e-04,  1.8915e-03,  3.1618e-03,
          2.0772e-03, -3.0952e-05,  2.0003e-03,  1.5412e-03, -8.8597e-07,
         -3.0585e-05,  8.4898e-03,  1.8140e-02,  1.7132e-02,  3.1278e-02,
          3.4201e-03, -3.3131e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -2.2641e-06, -1.2010e-05,  5.0222e-03,  8.5727e-03,  1.1854e-02,
          1.4479e-02,  2.4024e-03, -1.0699e-05,  1.1789e-02, -1.0040e-05,
         -2.1196e-05,  2.2612e-02,  1.4758e-02, -9.5491e-05,  2.5282e-02,
          4.6241e-02,  6.6831e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
         -1.0425e-05, -6.6604e-06,  1.4631e-02,  1.4648e-02,  4.0102e-03,
          3.1007e-02,  2.6710e-02, -8.3633e-06,  1.3558e-02, -1.8767e-05,
          8.5977e-03, -9.0899e-05,  2.1974e-02,  3.3194e-02,  2.1442e-02,
          3.2196e-02,  2.4185e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2149e-06,
         -2.2441e-05,  4.0556e-03,  1.1570e-02,  1.1568e-02,  1.3496e-02,
          1.8853e-02,  2.5196e-02,  2.0356e-02,  1.8486e-02,  3.1124e-03,
          1.8889e-02,  2.1006e-02,  2.8693e-02, -3.9975e-05,  6.4301e-03,
          4.0372e-02,  3.1899e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5751e-06, -1.2045e-05,
          1.2826e-03,  1.4207e-02,  5.3328e-03,  2.5743e-02,  1.8125e-02,
          9.7439e-03,  3.3327e-02,  2.6362e-02,  1.9336e-02,  1.6030e-02,
          4.2237e-03,  3.2533e-02,  4.6246e-02, -5.3166e-05,  3.6413e-02,
          4.1995e-02,  3.2252e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -9.4177e-06, -1.4321e-05,
          1.4571e-02,  1.1063e-02,  2.7668e-02,  3.2145e-02, -4.4958e-05,
          2.0671e-02,  4.5014e-02,  2.9330e-02,  1.4387e-02,  6.3401e-03,
          2.2203e-02,  4.3463e-02, -2.6969e-06,  2.4616e-02,  4.0782e-02,
          3.9292e-02,  2.3375e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.3449e-05,  2.6756e-03,
          1.5396e-02,  2.0318e-02,  2.5161e-02,  3.1271e-02,  1.8852e-02,
          4.9759e-02,  3.0206e-02,  2.7230e-02,  2.5471e-02,  1.6488e-02,
          3.3205e-02,  3.0492e-02, -1.1255e-04,  1.4226e-02,  4.2720e-02,
          3.5585e-02,  8.8315e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.6824e-05,  6.9318e-03,
         -3.5223e-05,  2.7965e-02,  3.1218e-02, -1.1399e-04,  2.5202e-02,
          3.1732e-02,  3.7200e-02,  3.1043e-02,  2.7077e-03,  2.9197e-02,
          4.2754e-02, -1.4799e-04,  2.7940e-02,  2.7969e-02,  4.1302e-02,
          1.7752e-02,  3.8831e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.2896e-03, -3.6919e-05,
          1.8604e-02,  3.0254e-02,  2.3865e-03,  3.1577e-02,  5.0446e-02,
          8.0181e-02,  5.2993e-02,  3.8188e-02,  4.7188e-02,  1.7369e-02,
          2.6597e-02,  1.7463e-02,  9.4019e-03,  3.5252e-02,  3.1337e-02,
          6.4751e-03,  3.7039e-06],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5091e-03,  4.3383e-03,
          1.7524e-02,  1.2628e-02,  9.4638e-04,  2.5550e-02,  5.8417e-02,
          4.5689e-02,  5.6326e-02,  3.2861e-02,  8.1544e-02,  5.3312e-02,
         -1.9015e-04,  1.0994e-02,  2.4306e-02,  3.6301e-02,  6.2666e-03,
          2.0616e-03, -3.1546e-06],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0951e-03,  9.2860e-03,
          1.7267e-02,  1.3294e-02,  1.0330e-02,  2.3575e-03,  2.3865e-02,
          6.2219e-02,  8.4761e-02,  6.6549e-02,  7.0562e-02, -2.6067e-04,
          3.0065e-02,  1.2025e-02,  3.8732e-02,  1.1220e-02,  3.7477e-03,
         -3.4052e-06, -8.9111e-07],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9879e-05, -2.0522e-05,
          3.5687e-03,  1.5783e-02,  1.7169e-02,  3.0676e-02,  3.8732e-02,
          3.2244e-02,  2.7227e-02,  2.8282e-02,  2.8482e-03,  3.8125e-02,
          1.7357e-02,  3.8514e-02,  2.3325e-02,  4.0698e-03,  2.2075e-04,
         -2.2472e-06,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -8.4295e-06, -5.2576e-05,
         -1.0404e-04, -6.6108e-05,  9.4731e-03,  1.1967e-02,  5.7030e-02,
          6.7094e-02,  7.8387e-02,  5.0495e-02,  8.0367e-03,  8.8120e-03,
          4.0583e-02,  3.0951e-02,  5.8200e-03,  6.7862e-04, -4.2231e-06,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -6.8909e-06, -5.1370e-07,
          1.6546e-03, -2.5438e-05, -1.0373e-04,  1.6532e-02,  6.2756e-03,
          3.0005e-02,  2.8399e-02, -2.0484e-04,  1.8772e-02,  2.0008e-02,
          3.8778e-02,  1.0171e-02,  2.5544e-03, -6.7610e-06,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.9282e-06,
         -1.6922e-05,  4.7968e-03,  2.6809e-02,  2.8340e-03,  1.9597e-02,
          3.5599e-02, -2.3722e-04,  1.6064e-02,  7.4845e-03,  4.7591e-02,
          2.1241e-02,  4.8341e-03,  2.8259e-04, -7.3614e-07,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -5.4142e-07, -1.2772e-05,
          8.6857e-04,  1.6607e-02,  2.0360e-03,  2.8524e-02,  5.3756e-02,
         -1.1003e-04,  7.9575e-03,  1.0424e-02,  3.8001e-02,  2.4560e-02,
          3.6397e-03,  8.2008e-04, -4.1069e-06,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -4.9043e-06, -2.2925e-05,
          9.0603e-03,  8.2924e-03,  2.5672e-02,  4.6912e-02,  1.0382e-02,
          1.1338e-02,  6.4925e-03,  2.6204e-02,  2.7714e-02,  5.3661e-03,
          1.2390e-03, -6.5095e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.9376e-07, -1.6932e-05,  1.4251e-03,
          1.3734e-02,  1.8361e-02,  3.7119e-02,  2.8306e-02, -6.5121e-05,
          1.1777e-02,  1.4990e-02,  3.4704e-02,  8.2450e-03,  2.8291e-03,
         -5.8157e-06, -1.9372e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3325e-06, -2.7247e-05,  8.7090e-03,
          6.5781e-03,  3.2960e-02,  3.1290e-02, -1.5443e-04,  2.9767e-02,
          7.4274e-03,  4.1662e-02,  1.1046e-02,  3.9021e-03,  2.0514e-05,
         -1.6272e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.0976e-07,  1.3838e-03,  6.1326e-03,
          2.0088e-02,  3.5365e-02,  1.3771e-02,  3.1831e-03,  1.2662e-02,
          3.0088e-02,  2.5211e-02,  3.8875e-03,  2.3658e-04, -3.8357e-06,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.5442e-04,  4.5744e-03, -9.1419e-05,
          2.7781e-02,  2.5100e-02, -1.0895e-04,  2.6411e-02,  4.0430e-02,
          2.7507e-02,  8.3686e-03,  5.4532e-04, -4.8898e-06,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.9957e-04,  6.7510e-03,  7.3411e-03,
          2.1607e-02, -9.8663e-05,  8.3445e-03,  4.0330e-03,  3.1937e-02,
          1.8678e-02,  5.1299e-03, -6.0541e-06, -1.1623e-07,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00]])







layer_error_after_calcul
torch.Size([8, 24, 24])
tensor([[[-0.0348, -0.0743, -0.0093,  ...,  0.0282, -0.0119, -0.0142],
         [ 0.0289, -0.1249,  0.0223,  ..., -0.0363, -0.0088, -0.0269],
         [ 0.0168,  0.0126, -0.0477,  ..., -0.0679, -0.0321,  0.0250],
         ...,
         [ 0.0165,  0.0151,  0.0469,  ..., -0.0766, -0.0318, -0.0058],
         [ 0.0192, -0.0668, -0.0489,  ..., -0.0024, -0.0114,  0.0489],
         [-0.0104, -0.0058,  0.0013,  ..., -0.0447,  0.0146,  0.0465]],

        [[ 0.0162,  0.0011, -0.0177,  ...,  0.0013, -0.0267, -0.0153],
         [ 0.0374,  0.0022, -0.0484,  ..., -0.0724, -0.0537,  0.0197],
         [ 0.0132,  0.0741,  0.0022,  ...,  0.1126,  0.0931,  0.0266],
         ...,
         [ 0.0037, -0.0650,  0.0201,  ...,  0.0607, -0.0384, -0.0369],
         [-0.0148,  0.0539,  0.0078,  ...,  0.0218,  0.0026, -0.0324],
         [ 0.0266,  0.0471,  0.0134,  ...,  0.0765, -0.0619,  0.0277]],

        [[-0.0725, -0.0507, -0.0034,  ..., -0.0257,  0.0087, -0.0153],
         [ 0.0772, -0.0060,  0.0351,  ..., -0.0085,  0.0627,  0.0271],
         [ 0.0648,  0.0017,  0.0589,  ..., -0.1376, -0.0609, -0.0178],
         ...,
         [-0.0206,  0.0036,  0.0173,  ..., -0.0404,  0.1073,  0.0022],
         [ 0.0042, -0.0262, -0.0619,  ..., -0.0682,  0.0348, -0.0226],
         [ 0.0200, -0.0080,  0.0072,  ..., -0.0674,  0.0314, -0.0386]],

        ...,

        [[ 0.0017,  0.0016,  0.0478,  ..., -0.0054, -0.0165, -0.0102],
         [-0.0051,  0.0907,  0.1388,  ...,  0.0611, -0.0164,  0.0394],
         [ 0.0563,  0.0183, -0.0669,  ..., -0.0236,  0.0373, -0.0153],
         ...,
         [-0.0031,  0.0341,  0.0042,  ..., -0.0564, -0.1094, -0.0104],
         [ 0.0175,  0.0655,  0.0234,  ...,  0.0922,  0.0535, -0.0192],
         [-0.0028, -0.0365, -0.0108,  ..., -0.0677,  0.0550, -0.0358]],

        [[ 0.0083, -0.0368, -0.0268,  ...,  0.0544, -0.0014, -0.0293],
         [-0.0331,  0.0101, -0.0337,  ...,  0.0101, -0.0435,  0.0543],
         [-0.0491, -0.0591,  0.0286,  ...,  0.0656,  0.0284,  0.0572],
         ...,
         [-0.0054, -0.0883, -0.0829,  ...,  0.1529, -0.0730, -0.0571],
         [-0.0332, -0.0336,  0.0233,  ...,  0.0316,  0.0205, -0.0027],
         [-0.0017, -0.0254, -0.0221,  ...,  0.0750, -0.0130,  0.0263]],

        [[ 0.0391,  0.0039, -0.0073,  ..., -0.0034,  0.0151,  0.0068],
         [-0.0429, -0.0621,  0.0156,  ...,  0.0852, -0.0996,  0.0153],
         [ 0.0387,  0.0132,  0.0076,  ...,  0.0052, -0.0073,  0.0407],
         ...,
         [ 0.0348,  0.0202, -0.0816,  ...,  0.0024,  0.0506, -0.0249],
         [ 0.0196,  0.0591,  0.0234,  ...,  0.0101, -0.0167, -0.0122],
         [-0.0120, -0.0129,  0.0169,  ...,  0.0314,  0.0404, -0.0015]]])
Bacw conv time:  0.10112166404724121


layer_error_after_calcul
torch.Size([4, 26, 26])
tensor([[[-8.8572e-05, -1.4652e-04,  4.1759e-05,  ...,  4.3249e-05,
           8.5065e-05,  4.7641e-05],
         [ 1.9873e-04,  1.8829e-04, -9.5808e-05,  ..., -2.1133e-03,
           1.6045e-03, -3.6904e-05],
         [ 9.1295e-05, -3.0506e-04, -3.1903e-04,  ...,  1.1478e-02,
          -1.0980e-02, -6.9408e-05],
         ...,
         [ 1.4095e-04,  1.4999e-04,  2.5177e-05,  ...,  6.6084e-04,
          -2.3363e-04, -2.1846e-04],
         [ 6.1103e-05,  2.3726e-04,  4.6000e-04,  ...,  1.2178e-04,
           3.3333e-04,  3.3633e-05],
         [-8.3017e-05, -5.3131e-05,  1.8640e-04,  ..., -1.5805e-04,
          -1.0985e-04, -5.4140e-07]],

        [[ 1.7798e-07,  9.6618e-05, -1.3021e-04,  ...,  1.4359e-04,
          -4.2128e-05, -7.8306e-05],
         [-2.1526e-05, -7.2114e-05, -5.7941e-05,  ..., -5.3484e-03,
           1.2498e-03, -2.9836e-05],
         [-5.0269e-05, -7.4904e-05, -6.4602e-04,  ..., -2.3939e-03,
           2.3907e-03,  1.4102e-04],
         ...,
         [ 1.8702e-05, -2.7777e-04,  1.8667e-05,  ...,  1.1877e-03,
           2.7879e-04, -4.2341e-05],
         [-9.8533e-05,  2.2535e-04, -8.0994e-06,  ...,  2.7504e-05,
           1.1837e-04, -2.7270e-04],
         [ 4.3066e-05, -7.7034e-05,  5.4028e-05,  ...,  1.1991e-04,
           4.6800e-05, -2.4362e-05]],

        [[-1.7004e-05, -1.2658e-04,  2.7739e-04,  ..., -4.3680e-05,
           8.9013e-05,  4.3786e-05],
         [ 1.3478e-04,  1.1714e-04, -3.8580e-04,  ...,  5.6832e-03,
          -1.4321e-02,  1.8670e-04],
         [-2.3131e-05, -2.3433e-04,  9.1532e-04,  ..., -1.6607e-02,
           2.2557e-02, -7.0869e-05],
         ...,
         [ 3.0989e-05, -6.7073e-05,  6.4984e-04,  ..., -1.1321e-04,
          -4.1516e-04,  1.1190e-04],
         [-4.3698e-05, -4.1879e-04,  2.7861e-05,  ...,  2.9854e-04,
          -3.2518e-04,  1.3667e-04],
         [-1.0155e-04, -5.4119e-05,  2.3639e-04,  ..., -1.7123e-04,
          -3.0665e-05, -8.1506e-06]],

        [[ 3.0425e-05, -7.2773e-05,  1.8657e-04,  ..., -1.3347e-04,
           3.3109e-05,  3.0949e-05],
         [ 1.9905e-04, -1.3569e-04, -1.8007e-04,  ..., -1.1291e-02,
          -1.3417e-02, -1.3189e-04],
         [-3.8014e-04,  1.0048e-04,  1.0649e-04,  ..., -7.9727e-03,
          -3.2873e-03, -3.6472e-05],
         ...,
         [ 3.7810e-05,  3.9011e-05, -9.4754e-05,  ...,  5.4424e-04,
           4.0022e-06,  4.9444e-05],
         [ 1.9669e-05,  1.5893e-05,  1.4230e-04,  ..., -1.7588e-04,
           9.6954e-05, -2.7144e-05],
         [ 2.5352e-05, -2.0988e-06,  4.4933e-05,  ...,  4.8817e-05,
          -3.6138e-05, -3.2487e-05]]])
Bacw conv time:  0.057779550552368164



Bacw conv time:  0.006426334381103516
Bacw conv time:  0.00655055046081543
Bacw conv time:  0.005704402923583984
Bacw conv time:  0.006474018096923828
Bacw conv time:  0.005749702453613281
Bacw conv time:  0.00643467903137207


Epoch: 1/15, train loss: 1.9339, train Accuracy: 0.7113, test loss: 1.7944, test Accuracy: 0.87, epoch time: 63.504s
Epoch: 2/15, train loss: 1.6755, train Accuracy: 0.9337, test loss: 1.7513, test Accuracy: 0.885, epoch time: 63.841s
Epoch: 3/15, train loss: 1.5844, train Accuracy: 0.98, test loss: 1.6537, test Accuracy: 0.895, epoch time: 62.797s
Epoch: 4/15, train loss: 1.5443, train Accuracy: 0.9962, test loss: 1.6674, test Accuracy: 0.905, epoch time: 61.012s


Попробовать поварировать batch size и также нужно обічную нейронку попробовать улучшить чтоб понять что при таких то данніх мі больше віжать из нее не можем и тогда нужно будет улучшать конв нейронку


убедится что точно работает как и со старім форвардом для разніх фильтров не только self.a[0]