{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9489d3dc-1b5a-41ff-bda3-3553d9a3963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from src.model.activation import Relu, LeakyRelu, Linear, Sigmoid\n",
    "from src.model.base import NeuralNetwork\n",
    "from src.model.layer import Convolutional, Input3D, Input, FullyConnected, Flatten\n",
    "from src.model.loss import CategoricalCrossEntropy\n",
    "from src.model.metric import AccuracyOneHot\n",
    "from src.model.optimizer import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d11bf4-d095-4e28-8637-67a1856f7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = [\n",
    "#     {\n",
    "#         'input': [\n",
    "#             [\n",
    "#                 [0.0, 1.0, 1.0, 1.0, 0.0],\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#                 [0.0, 1.0, 1.0, 1.0, 0.0],\n",
    "#             ]\n",
    "#         ],\n",
    "#         'output': [1.0, 0.0]\n",
    "#     },\n",
    "#     {\n",
    "#         'input': [\n",
    "#             [\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#                 [0.0, 1.0, 0.0, 1.0, 0.0],\n",
    "#                 [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "#                 [0.0, 1.0, 0.0, 1.0, 0.0],\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#             ]\n",
    "#         ],\n",
    "#         'output': [0.0, 1.0]\n",
    "#     }    \n",
    "# ]\n",
    "\n",
    "# test_data = [\n",
    "#     {\n",
    "#         'input': [\n",
    "#             [\n",
    "#                 [1.0, 0.0, 1.0, 1.0, 0.0],\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#                 [0.0, 1.0, 1.0, 1.0, 0.0],\n",
    "#             ]\n",
    "#         ],\n",
    "#         'output': [1.0, 0.0]\n",
    "#     },\n",
    "#     {\n",
    "#         'input': [\n",
    "#             [\n",
    "#                 [0.0, 1.0, 0.0, 0.0, 1.0],\n",
    "#                 [0.0, 1.0, 0.0, 1.0, 0.0],\n",
    "#                 [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "#                 [0.0, 1.0, 0.0, 1.0, 0.0],\n",
    "#                 [1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#             ]\n",
    "#         ],\n",
    "#         'output': [0.0, 1.0]\n",
    "#     }\n",
    "    \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d60aa8-1cf7-4615-bf73-8ab7ebb8a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = [\n",
    "#     Convolutional(LeakyRelu(), filters_num=2, kernel_size=2),\n",
    "#     Convolutional(LeakyRelu(), filters_num=2, kernel_size=2),\n",
    "#     Flatten(),\n",
    "#     #FullyConnected(10, LeakyRelu()),\n",
    "#     FullyConnected(2, Linear())\n",
    "# ]\n",
    "# nn = NeuralNetwork(\n",
    "#     Input3D((1, 5, 5)),\n",
    "#     layers,\n",
    "#     optimizer=SGD(),\n",
    "#     loss=CategoricalCrossEntropy(),\n",
    "#     metric=AccuracyOneHot(),\n",
    "#     convert_prediction='argmax'\n",
    "# )\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "# epochs = 10\n",
    "# nn.fit(train_data, test_data, epochs=epochs, verbose=True)\n",
    "# print('exec time: ', time.time() - start_time)\n",
    "\n",
    "# print('test_111')\n",
    "# import sys\n",
    "# sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6165230-5a9d-406a-bd91-f5aa26ac0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(image, digit):\n",
    "    image = torch.tensor(image).numpy().reshape(28, 28)\n",
    "    plt.figure()\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(f\"Predicted digit: {digit}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ccd756-2412-425e-9555-74e037d8ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_digit(onehot_array):\n",
    "    for index, value in enumerate(onehot_array):\n",
    "        if value == 1.0:\n",
    "            return index\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df8f04f-cc51-4278-b104-832dbab927ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_digit(digit):\n",
    "    output = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    return output[-digit:] + output[:-digit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d4a346-884e-494b-bf37-4f4e65506db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_digits_data():\n",
    "\n",
    "#     train_dataset = []\n",
    "\n",
    "#     train = pd.read_csv(\"../data/digits/train.csv\", header=0, nrows=5000)\n",
    "\n",
    "#     for index in train.index:\n",
    "#         input_values = [\n",
    "#             float(val) / 255.0 for val in train.loc[index].drop(\"label\").values\n",
    "#         ]\n",
    "#         train_dataset.append(\n",
    "#             {\n",
    "#                 \"input\": input_values,\n",
    "#                 \"output\": get_onehot_digit(int(train.loc[index][\"label\"])),\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     random.seed(1)\n",
    "#     random.shuffle(train_dataset)\n",
    "#     return train_dataset[:800], train_dataset[800:1000]\n",
    "\n",
    "# train_dataset, test_dataset = get_digits_data()\n",
    "\n",
    "# layers = [\n",
    "#     FullyConnected(256, LeakyRelu()),\n",
    "#     FullyConnected(128, LeakyRelu()),\n",
    "#     FullyConnected(64, LeakyRelu()),\n",
    "#     FullyConnected(10, Linear()),\n",
    "# ]\n",
    "# nn = NeuralNetwork(\n",
    "#     Input(784),\n",
    "#     layers,\n",
    "#     optimizer=Adam(learning_rate=0.001),\n",
    "#     loss=CategoricalCrossEntropy(),\n",
    "#     metric=AccuracyOneHot(),\n",
    "#     convert_prediction=\"argmax\",\n",
    "#     on_cuda=False\n",
    "# )\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# epochs = 15\n",
    "# history = nn.fit(train_dataset, test_dataset, epochs=epochs, batch_size=1, verbose=True)\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# prediction = nn.predict(test_dataset)\n",
    "\n",
    "# for index, predicted_digit in enumerate(prediction[:10]):\n",
    "#     plot_digit(test_dataset[index]['input'], get_digit(predicted_digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20fe48b6-c14d-4b7c-82fe-0a38333aee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit_cnn(image, digit):\n",
    "    image = torch.tensor(image).numpy()\n",
    "    plt.figure()\n",
    "    plt.title(f\"Predicted digit: {digit}\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e065054b-ca30-496a-a976-f69a0b4157a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_digits_data_cnn():\n",
    "\n",
    "    train_dataset = []\n",
    "\n",
    "    train = pd.read_csv(\"../data/digits/train.csv\", header=0, nrows=5000)\n",
    "\n",
    "    for index in train.index:\n",
    "        input_values = [\n",
    "            float(val) / 255.0 for val in train.loc[index].drop(\"label\").values\n",
    "        ]\n",
    "        train_dataset.append(\n",
    "            {\n",
    "                \"input\": [torch.tensor(input_values).reshape(28, 28).tolist()],\n",
    "                \"output\": get_onehot_digit(int(train.loc[index][\"label\"])),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # print(train_dataset[:2])\n",
    "    # import sys\n",
    "    # sys.exit(1)\n",
    "    \n",
    "    random.seed(1)\n",
    "    random.shuffle(train_dataset)\n",
    "    return train_dataset[:800], train_dataset[800:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d08b65-bb36-4e2a-b4b9-a54c97d2210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15, train loss: 1.9087, train Accuracy: 0.7275, test loss: 1.7215, test Accuracy: 0.855, epoch time: 50.854s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     24\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexec time: \u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m     28\u001b[0m prediction \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mpredict(test_data)\n",
      "File \u001b[0;32m~/Documents/Projects/vladk-neural-network/src/model/base.py:691\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, train_dataset, test_dataset, epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m    685\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    686\u001b[0m     train_dataset[k : k \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataset), batch_size)\n\u001b[1;32m    688\u001b[0m ]\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m--> 691\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__prediction)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__actual \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__actual)\n",
      "File \u001b[0;32m~/Documents/Projects/vladk-neural-network/src/model/base.py:651\u001b[0m, in \u001b[0;36mNeuralNetwork.__process_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39ma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_data)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(input_data), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 651\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__prediction\u001b[38;5;241m.\u001b[39mappend(predict)\n\u001b[1;32m    655\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/vladk-neural-network/src/model/base.py:85\u001b[0m, in \u001b[0;36mNeuralNetwork.__forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m layer_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__layers):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__layers[layer_index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39ma\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     layer_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39ma\n",
      "File \u001b[0;32m~/Documents/Projects/vladk-neural-network/src/model/layer.py:342\u001b[0m, in \u001b[0;36mConvolutional.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    340\u001b[0m     start_for_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# print('Forward time: ', time.time() - start_for_time)\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# print('self.a[0]')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# time.sleep(1)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/vladk-neural-network/src/model/layer.py:331\u001b[0m, in \u001b[0;36mConvolutional._convolution\u001b[0;34m(self, input, filters, biases, output_c, output_h, output_w, input_c, kernel_size)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output_h):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output_w):\n\u001b[0;32m--> 331\u001b[0m         separate_regions[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[:, i:i \u001b[38;5;241m+\u001b[39m kernel_size, j:j \u001b[38;5;241m+\u001b[39m kernel_size]\n\u001b[1;32m    333\u001b[0m expanded_regions \u001b[38;5;241m=\u001b[39m separate_regions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    334\u001b[0m duplicated_separate_regions \u001b[38;5;241m=\u001b[39m expanded_regions\u001b[38;5;241m.\u001b[39mexpand(output_c, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data, test_data = get_digits_data_cnn()\n",
    "\n",
    "layers = [\n",
    "    Convolutional(LeakyRelu(), filters_num=4, kernel_size=4),\n",
    "    Convolutional(LeakyRelu(), filters_num=8, kernel_size=3),\n",
    "    Convolutional(LeakyRelu(), filters_num=16, kernel_size=2),\n",
    "    Flatten(),\n",
    "    FullyConnected(128, LeakyRelu()),\n",
    "    FullyConnected(10, Linear())\n",
    "]\n",
    "nn = NeuralNetwork(\n",
    "    Input3D((1, 28, 28)),\n",
    "    layers,\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=CategoricalCrossEntropy(),\n",
    "    metric=AccuracyOneHot(),\n",
    "    convert_prediction='argmax',\n",
    "    on_cuda=False\n",
    ")\n",
    "\n",
    "#plot_digit(train_data[0]['input'][0])\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 15\n",
    "nn.fit(train_data, test_data, epochs=epochs, batch_size=1, verbose=True)\n",
    "print('exec time: ', time.time() - start_time)\n",
    "\n",
    "prediction = nn.predict(test_data)\n",
    "\n",
    "for index, predicted_digit in enumerate(prediction[:10]):\n",
    "    plot_digit_cnn(test_data[index]['input'][0], get_digit(predicted_digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae876ade-c60d-4799-967d-ad3e847b8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = [\n",
    "#     {\n",
    "#         'input': [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0],\n",
    "#         'output': [1.0, 0.0]\n",
    "#     },\n",
    "#     {\n",
    "#         'input': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#         'output': [0.0, 1.0]\n",
    "#     }    \n",
    "# ]\n",
    "\n",
    "# test_data = [\n",
    "#     {\n",
    "#         'input': [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0],\n",
    "#         'output': [1.0, 0.0]\n",
    "#     },\n",
    "#     {\n",
    "#         'input': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "#         'output': [0.0, 1.0]\n",
    "#     }    \n",
    "# ]\n",
    "\n",
    "# layers = [\n",
    "#     FullyConnected(25, LeakyRelu()),\n",
    "#     FullyConnected(10, LeakyRelu()),\n",
    "#     FullyConnected(2, Linear())\n",
    "# ]\n",
    "# nn = NeuralNetwork(\n",
    "#     Input(25),\n",
    "#     layers,\n",
    "#     optimizer=SGD(),\n",
    "#     loss=CategoricalCrossEntropy(),\n",
    "#     metric=AccuracyOneHot(),\n",
    "#     convert_prediction='argmax'\n",
    "# )\n",
    "\n",
    "# start_time = time.time()\n",
    "# epochs = 200\n",
    "# nn.fit(train_data, test_data, epochs=epochs, verbose=True)\n",
    "# print('exec time: ', time.time() - start_time)\n",
    "\n",
    "# print('test_222')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
